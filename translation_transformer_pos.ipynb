{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q68b5n-xSPjS"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import enum\n",
    "import argparse\n",
    "\n",
    "# Visualization related imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torchtext\n",
    "# !conda install -y torchtext==0.8.0 -c pytorch\n",
    "# !pip install torchtext==0.8.1\n",
    "# !pip uninstall torchtext -y\n",
    "# !pip install torchtext\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpm2JqRJSPjY"
   },
   "source": [
    "\n",
    "# Language Translation with ``nn.Transformer`` and torchtext\n",
    "\n",
    "This tutorial shows:\n",
    "    - How to train a translation model from scratch using Transformer.\n",
    "    - Use torchtext library to access  [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1)_ dataset to train a German to English translation model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BlWsNJWSPja"
   },
   "source": [
    "## Data Sourcing and Processing\n",
    "\n",
    "[torchtext library](https://pytorch.org/text/stable/)_ has utilities for creating datasets that can be easily\n",
    "iterated through for the purposes of creating a language translation\n",
    "model. In this example, we show how to use torchtext's inbuilt datasets,\n",
    "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
    "[Multi30k dataset from torchtext library](https://pytorch.org/text/stable/datasets.html#multi30k)_\n",
    "that yields a pair of source-target raw sentences.\n",
    "\n",
    "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pseXrfsLSPjb"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'de'\n",
    "\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = '<bos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkMFoV3lSYE9",
    "outputId": "5d4fd060-121c-4261-b6ad-f33374f8cdc5"
   },
   "outputs": [],
   "source": [
    "# !pip install -U torchdata\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWmveW2LSPjd"
   },
   "source": [
    "Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "\n",
    "```python\n",
    "pip install -U torchdata\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "9CBZ7JYdUa_G",
    "outputId": "c0936a3a-3284-4cae-e513-524934f4f750"
   },
   "outputs": [],
   "source": [
    "# !pip install -U portalocker\n",
    "import portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cK30YukESPjd"
   },
   "outputs": [],
   "source": [
    "# token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "# token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WTurMcwSPjg"
   },
   "source": [
    "## Seq2Seq Network using Transformer\n",
    "\n",
    "Transformer is a Seq2Seq model introduced in [“Attention is all you\n",
    "need”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)_\n",
    "paper for solving machine translation tasks.\n",
    "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
    "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
    "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
    "encodings to provide position information of input tokens to the model. The second part is the\n",
    "actual [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ model.\n",
    "Finally, the output of the Transformer model is passed through linear layer\n",
    "that gives unnormalized probabilities for each token in the target language.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vDYRvCN-SPji"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, src_vocab_size, trg_vocab_size, number_of_heads, number_of_layers, dropout_probability, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeds source/target token ids into embedding vectors\n",
    "        self.src_embedding = Embedding(src_vocab_size, model_dimension)\n",
    "        self.trg_embedding = Embedding(trg_vocab_size, model_dimension)\n",
    "\n",
    "        # Adds positional information to source/target token's embedding vector\n",
    "        # (otherwise we'd lose the positional information which is important in human languages)\n",
    "        self.src_pos_embedding = PositionalEncoding(model_dimension, dropout_probability)\n",
    "        self.trg_pos_embedding = PositionalEncoding(model_dimension, dropout_probability)\n",
    "\n",
    "        # All of these will get deep-copied multiple times internally\n",
    "        if attn_type == 'qkv':\n",
    "            mha = MultiHeadedAttention(model_dimension, number_of_heads, dropout_probability, log_attention_weights)\n",
    "        else:\n",
    "            mha = MultiHeadedAttention_KV(model_dimension, number_of_heads, dropout_probability, log_attention_weights)\n",
    "        pwn = PositionwiseFeedForwardNet(model_dimension, dropout_probability)\n",
    "        encoder_layer = EncoderLayer(model_dimension, dropout_probability, mha, pwn)\n",
    "        decoder_layer = DecoderLayer(model_dimension, dropout_probability, mha, pwn)\n",
    "\n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder = Encoder(encoder_layer, number_of_layers)\n",
    "        self.decoder = Decoder(decoder_layer, number_of_layers)\n",
    "\n",
    "        # Converts final target token representations into log probability vectors of trg_vocab_size dimensionality\n",
    "        # Why log? -> PyTorch's nn.KLDivLoss expects log probabilities\n",
    "        self.decoder_generator = DecoderGenerator(model_dimension, trg_vocab_size)\n",
    "        \n",
    "        self.init_params()\n",
    "\n",
    "    # This part wasn't mentioned in the paper, but it's super important!\n",
    "    def init_params(self):\n",
    "        # I tested both PyTorch's default initialization and this, and xavier has tremendous impact! I didn't expect\n",
    "        # that the model's perf, with normalization layers, is so dependent on the choice of weight initialization.\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src_token_ids_batch, trg_token_ids_batch, src_mask, trg_mask):\n",
    "        src_representations_batch = self.encode(src_token_ids_batch, src_mask)\n",
    "        trg_log_probs = self.decode(trg_token_ids_batch, src_representations_batch, trg_mask, src_mask)\n",
    "        \n",
    "        return trg_log_probs\n",
    "\n",
    "    # Modularize into encode/decode functions for optimizing the decoding/translation process (we'll get to it later)\n",
    "    def encode(self, src_token_ids_batch, src_mask):\n",
    "        # Shape = (B, S, D) , where B - batch size, S - longest source token-sequence length and D - model dimension\n",
    "        # The whole encoder stack perserves this shape\n",
    "        src_embeddings_batch = self.src_embedding(src_token_ids_batch)  # get embedding vectors for src token ids\n",
    "        src_embeddings_batch = self.src_pos_embedding(src_embeddings_batch)  # add positional embedding\n",
    "        src_representations_batch = self.encoder(src_embeddings_batch, src_mask)  # forward pass through the encoder\n",
    "\n",
    "        return src_representations_batch\n",
    "\n",
    "    def decode(self, trg_token_ids_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        trg_embeddings_batch = self.trg_embedding(trg_token_ids_batch)  # get embedding vectors for trg token ids\n",
    "        trg_embeddings_batch = self.trg_pos_embedding(trg_embeddings_batch)  # add positional embedding\n",
    "        \n",
    "        # Shape (B, T, D), where B - batch size, T - longest target token-sequence length and D - model dimension\n",
    "        trg_representations_batch = self.decoder(trg_embeddings_batch, src_representations_batch, trg_mask, src_mask)\n",
    "\n",
    "        # After this line we'll have a shape (B, T, V), where V - target vocab size, \n",
    "        # decoder generator does a simple linear projection followed by log softmax\n",
    "        trg_log_probs = self.decoder_generator(trg_representations_batch)\n",
    "\n",
    "        # Reshape into (B*T, V) as that's a suitable format for passing it into KL div loss\n",
    "        trg_log_probs = trg_log_probs.reshape(-1, trg_log_probs.shape[-1])\n",
    "\n",
    "        return trg_log_probs  # the reason I use log here is that PyTorch's nn.KLDivLoss expects log probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, number_of_layers):\n",
    "        super().__init__()\n",
    "        assert isinstance(encoder_layer, EncoderLayer), f'Expected EncoderLayer got {type(encoder_layer)}.'\n",
    "\n",
    "        # Get a list of 'number_of_layers' independent encoder layers\n",
    "        self.encoder_layers = get_clones(encoder_layer, number_of_layers)  \n",
    "        self.norm = nn.LayerNorm(encoder_layer.model_dimension)\n",
    "\n",
    "    def forward(self, src_embeddings_batch, src_mask):\n",
    "        # Just update the naming so as to reflect the semantics of what this var will become (the initial encoder layer\n",
    "        # has embedding vectors as input but later layers have richer token representations)\n",
    "        src_representations_batch = src_embeddings_batch\n",
    "\n",
    "        # Forward pass through the encoder stack\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # src_mask's role is to mask/ignore padded token representations in the multi-headed self-attention module\n",
    "            src_representations_batch = encoder_layer(src_representations_batch, src_mask)\n",
    "\n",
    "        # Not mentioned explicitly in the paper \n",
    "        # (a consequence of using LayerNorm before instead of after the SublayerLogic module)\n",
    "        return self.norm(src_representations_batch)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, dropout_probability, multi_headed_attention, pointwise_net):\n",
    "        super().__init__()\n",
    "        num_of_sublayers_encoder = 2\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dimension, dropout_probability), num_of_sublayers_encoder)\n",
    "\n",
    "        self.multi_headed_attention = multi_headed_attention\n",
    "        self.pointwise_net = pointwise_net\n",
    "\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "    def forward(self, src_representations_batch, src_mask):\n",
    "        # Define an anonymous (lambda) function which only takes src_representations_batch (srb) as input,\n",
    "        # this way we have a uniform interface for the sublayer logic.\n",
    "        encoder_self_attention = lambda srb: self.multi_headed_attention(query=srb, key=srb, value=srb, mask=src_mask)\n",
    "\n",
    "        # Self-attention MHA sublayer followed by point-wise feed forward net sublayer\n",
    "        # SublayerLogic takes as the input the data and the logic it should execute (attention/feedforward)\n",
    "        src_representations_batch = self.sublayers[0](src_representations_batch, encoder_self_attention)\n",
    "        src_representations_batch = self.sublayers[1](src_representations_batch, self.pointwise_net)\n",
    "\n",
    "        return src_representations_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, number_of_layers):\n",
    "        super().__init__()\n",
    "        assert isinstance(decoder_layer, DecoderLayer), f'Expected DecoderLayer got {type(decoder_layer)}.'\n",
    "\n",
    "        self.decoder_layers = get_clones(decoder_layer, number_of_layers)\n",
    "        self.norm = nn.LayerNorm(decoder_layer.model_dimension)\n",
    "\n",
    "    def forward(self, trg_embeddings_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        # Just update the naming so as to reflect the semantics of what this var will become\n",
    "        trg_representations_batch = trg_embeddings_batch\n",
    "\n",
    "        # Forward pass through the decoder stack\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            # Target mask masks pad tokens as well as future tokens (current target token can't look forward)\n",
    "            trg_representations_batch = decoder_layer(trg_representations_batch, src_representations_batch, trg_mask, src_mask)\n",
    "\n",
    "        # Not mentioned explicitly in the paper \n",
    "        # (a consequence of using LayerNorm before instead of after the SublayerLogic module)\n",
    "        return self.norm(trg_representations_batch)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, dropout_probability, multi_headed_attention, pointwise_net):\n",
    "        super().__init__()\n",
    "        num_of_sublayers_decoder = 3\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dimension, dropout_probability), num_of_sublayers_decoder)\n",
    "\n",
    "        self.trg_multi_headed_attention = copy.deepcopy(multi_headed_attention)\n",
    "        self.src_multi_headed_attention = copy.deepcopy(multi_headed_attention)\n",
    "        self.pointwise_net = pointwise_net\n",
    "\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "    def forward(self, trg_representations_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        # Define an anonymous (lambda) function which only takes trg_representations_batch (trb - funny name I know)\n",
    "        # as input - this way we have a uniform interface for the sublayer logic.\n",
    "        # The inputs which are not passed into lambdas (masks/srb) are \"cached\" here that's why the thing works.\n",
    "        srb = src_representations_batch  # simple/short alias\n",
    "        decoder_trg_self_attention = lambda trb: self.trg_multi_headed_attention(query=trb, key=trb, value=trb, mask=trg_mask)\n",
    "        decoder_src_attention = lambda trb: self.src_multi_headed_attention(query=trb, key=srb, value=srb, mask=src_mask)\n",
    "\n",
    "        # Self-attention MHA sublayer followed by a source-attending MHA and point-wise feed forward net sublayer\n",
    "        trg_representations_batch = self.sublayers[0](trg_representations_batch, decoder_trg_self_attention)\n",
    "        trg_representations_batch = self.sublayers[1](trg_representations_batch, decoder_src_attention)\n",
    "        trg_representations_batch = self.sublayers[2](trg_representations_batch, self.pointwise_net)\n",
    "\n",
    "        return trg_representations_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGenerator(nn.Module):\n",
    "    def __init__(self, model_dimension, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(model_dimension, vocab_size)\n",
    "\n",
    "        # -1 stands for apply the log-softmax along the last dimension i.e. over the vocab dimension as the output from\n",
    "        # the linear layer has shape (B, T, V), B - batch size, T - max target token-sequence, V - target vocab size\n",
    "        \n",
    "        # again using log softmax as PyTorch's nn.KLDivLoss expects log probabilities (just a technical detail)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, trg_representations_batch):\n",
    "        # Project from D (model dimension) into V (target vocab size) and apply the log softmax along V dimension\n",
    "        return self.log_softmax(self.linear(trg_representations_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerLogic(nn.Module):\n",
    "    def __init__(self, model_dimension, dropout_probability):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(model_dimension)\n",
    "        self.dropout = nn.Dropout(p=dropout_probability)\n",
    "\n",
    "    # Note: the original paper had LayerNorm AFTER the residual connection and addition operation\n",
    "    # multiple experiments I found showed that it's more effective to do it BEFORE, how did they figure out which one is\n",
    "    # better? Experiments! There is a similar thing in DCGAN and elsewhere.\n",
    "    def forward(self, representations_batch, sublayer_module):\n",
    "        # Residual connection between input and sublayer output, details: Page 7, Chapter 5.4 \"Regularization\",\n",
    "        return representations_batch + self.dropout(sublayer_module(self.norm(representations_batch)))\n",
    "\n",
    "\n",
    "class PositionwiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "        It's position-wise because this feed forward net will be independently applied to every token's representation.\n",
    "\n",
    "        This net will basically be applied independently to every token's representation (you can think of it as if\n",
    "        there was a nested for-loop going over the batch size and max token sequence length dimensions\n",
    "        and applied this network to token representations. PyTorch does this auto-magically behind the scenes.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dimension, dropout_probability, width_mult=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(model_dimension, width_mult * model_dimension)\n",
    "        self.linear2 = nn.Linear(width_mult * model_dimension, model_dimension)\n",
    "\n",
    "        # This dropout layer is not explicitly mentioned in the paper but it's common to use to avoid over-fitting\n",
    "        self.dropout = nn.Dropout(p=dropout_probability)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # Representations_batch's shape = (B - batch size, S/T - max token sequence length, D- model dimension).\n",
    "    def forward(self, representations_batch):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(representations_batch))))\n",
    "\n",
    "\n",
    "#\n",
    "# Input modules\n",
    "#\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dimension):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(vocab_size, model_dimension)\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "    def forward(self, token_ids_batch):\n",
    "        assert token_ids_batch.ndim == 2, f'Expected: (batch size, max token sequence length), got {token_ids_batch.shape}'\n",
    "\n",
    "        # token_ids_batch has shape (B, S/T), where B - batch size, S/T max src/trg token-sequence length\n",
    "        # Final shape will be (B, S/T, D) where D is the model dimension, every token id has associated vector\n",
    "        embeddings = self.embeddings_table(token_ids_batch)\n",
    "\n",
    "        # (stated in the paper) multiply the embedding weights by the square root of model dimension\n",
    "        # Page 5, Chapter 3.4 \"Embeddings and Softmax\"\n",
    "        return embeddings * math.sqrt(self.model_dimension)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, dropout_probability, expected_max_sequence_length=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        # (stated in the paper) Use sine functions whose frequencies form a geometric progression as position encodings,\n",
    "        # (learning encodings will also work so feel free to change it!). Page 6, Chapter 3.5 \"Positional Encoding\"\n",
    "        position_id = torch.arange(0, expected_max_sequence_length).unsqueeze(1)\n",
    "        frequencies = torch.pow(10000., -torch.arange(0, model_dimension, 2, dtype=torch.float) / model_dimension)\n",
    "\n",
    "        # Checkout playground.py for visualization of how these look like (it's super simple don't get scared)\n",
    "        positional_encodings_table = torch.zeros(expected_max_sequence_length, model_dimension)\n",
    "        positional_encodings_table[:, 0::2] = torch.sin(position_id * frequencies)  # sine on even positions\n",
    "        positional_encodings_table[:, 1::2] = torch.cos(position_id * frequencies)  # cosine on odd positions\n",
    "\n",
    "        # Register buffer because we want to save the positional encodings table inside state_dict even though\n",
    "        # these are not trainable (not model's parameters) so they otherwise would be excluded from the state_dict\n",
    "        self.register_buffer('positional_encodings_table', positional_encodings_table)\n",
    "\n",
    "    def forward(self, embeddings_batch):\n",
    "        assert embeddings_batch.ndim == 3 and embeddings_batch.shape[-1] == self.positional_encodings_table.shape[1], \\\n",
    "            f'Expected (batch size, max token sequence length, model dimension) got {embeddings_batch.shape}'\n",
    "\n",
    "        # embedding_batch's shape = (B, S/T, D), where S/T max src/trg token-sequence length, D - model dimension\n",
    "        # So here we get (S/T, D) shape which will get broad-casted to (B, S/T, D) when we try and add it to embeddings\n",
    "        positional_encodings = self.positional_encodings_table[:embeddings_batch.shape[1]]\n",
    "\n",
    "        # (stated in the paper) Applying dropout to the sum of positional encodings and token embeddings\n",
    "        # Page 7, Chapter 5.4 \"Regularization\"\n",
    "        return self.dropout(embeddings_batch + positional_encodings)\n",
    "\n",
    "\n",
    "#\n",
    "# Helper model functions\n",
    "#\n",
    "\n",
    "\n",
    "def get_clones(module, num_of_deep_copies):\n",
    "    # Create deep copies so that we can tweak each module's weights independently\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_of_deep_copies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        This module already exists in PyTorch. The reason I implemented it here from scratch is that\n",
    "        PyTorch implementation is super complicated as they made it as generic/robust as possible whereas\n",
    "        on the other hand I only want to support a limited use-case.\n",
    "\n",
    "        Also this is arguable the most important architectural component in the Transformer model.\n",
    "\n",
    "        Additional note:\n",
    "        This is conceptually super easy stuff. It's just that matrix implementation makes things a bit less intuitive.\n",
    "        If you take your time and go through the code and figure out all of the dimensions + write stuff down on paper\n",
    "        you'll understand everything. Also do check out this amazing blog for conceptual understanding:\n",
    "\n",
    "        https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "        Optimization notes:\n",
    "\n",
    "        qkv_nets could be replaced by Parameter(torch.empty(3 * model_dimension, model_dimension)) and one more matrix\n",
    "        for bias, which would make the implementation a bit more optimized. For the sake of easier understanding though,\n",
    "        I'm doing it like this - using 3 \"feed forward nets\" (without activation/identity hence the quotation marks).\n",
    "        Conceptually both implementations are the same.\n",
    "\n",
    "        PyTorch's query/key/value are of different shape namely (max token sequence length, batch size, model dimension)\n",
    "        whereas I'm using (batch size, max token sequence length, model dimension) because it's easier to understand\n",
    "        and consistent with computer vision apps (batch dimension is always first followed by the number of channels (C)\n",
    "        and image's spatial dimensions height (H) and width (W) -> (B, C, H, W).\n",
    "\n",
    "        This has an important optimization implication, they can reshape their matrix into (B*NH, S/T, HD)\n",
    "        (where B - batch size, S/T - max src/trg sequence length, NH - number of heads, HD - head dimension)\n",
    "        in a single step and I can only get to (B, NH, S/T, HD) in single step\n",
    "        (I could call contiguous() followed by view but that's expensive as it would incur additional matrix copy)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension, number_of_heads, dropout_probability, log_attention_weights):\n",
    "        super().__init__()\n",
    "        assert model_dimension % number_of_heads == 0, f'Model dimension must be divisible by the number of heads.'\n",
    "\n",
    "        self.head_dimension = int(model_dimension / number_of_heads)\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        self.qkv_nets = get_clones(nn.Linear(model_dimension, model_dimension), 3)  # identity activation hence \"nets\"\n",
    "        self.out_projection_net = nn.Linear(model_dimension, model_dimension)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(p=dropout_probability)  # no pun intended, not explicitly mentioned in paper\n",
    "        self.softmax = nn.Softmax(dim=-1)  # -1 stands for apply the softmax along the last dimension\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # should we log attention weights\n",
    "        self.attention_weights = None  # for visualization purposes, I cache the weights here (translation_script.py)\n",
    "        \n",
    "\n",
    "    def attention(self, query, key, value, mask):\n",
    "        # Step 1: Scaled dot-product attention, Page 4, Chapter 3.2.1 \"Scaled Dot-Product Attention\"\n",
    "        # Notation: B - batch size, S/T max src/trg token-sequence length, NH - number of heads, HD - head dimension\n",
    "        # query/key/value shape = (B, NH, S/T, HD), scores shape = (B, NH, S, S), (B, NH, T, T) or (B, NH, T, S)\n",
    "        # scores have different shapes as MHA is used in 3 contexts, self attention for src/trg and source attending MHA\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dimension)\n",
    "        \n",
    "\n",
    "        # Step 2: Optionally mask tokens whose representations we want to ignore by setting a big negative number\n",
    "        # to locations corresponding to those tokens (force softmax to output 0 probability on those locations).\n",
    "        # mask shape = (B, 1, 1, S) or (B, 1, T, T) will get broad-casted (copied) as needed to match scores shape\n",
    "\n",
    "        # if mask is not None:\n",
    "        scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n",
    "\n",
    "        # Step 3: Calculate the attention weights - how much should we attend to surrounding token representations\n",
    "        attention_weights = self.softmax(scores)\n",
    "        \n",
    "        # Step 4: Not defined in the original paper apply dropout to attention weights as well\n",
    "        attention_weights = self.attention_dropout(attention_weights)\n",
    "\n",
    "        # Step 5: based on attention weights calculate new token representations\n",
    "        # attention_weights shape = (B, NH, S, S)/(B, NH, T, T) or (B, NH, T, S), value shape = (B, NH, S/T, HD)\n",
    "        # Final shape (B, NH, S, HD) for source MHAs or (B, NH, T, HD) target MHAs (again MHAs are used in 3 contexts)\n",
    "        intermediate_token_representations = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return intermediate_token_representations, attention_weights  # attention weights for visualization purposes\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Step 1: Input linear projection\n",
    "        # Notation: B - batch size, NH - number of heads, S/T - max src/trg token-sequence length, HD - head dimension\n",
    "        # Shape goes from (B, S/T, NH*HD) over (B, S/T, NH, HD) to (B, NH, S/T, HD) (NH*HD=D where D is model dimension)\n",
    "        query, key, value = [net(x).view(batch_size, -1, self.number_of_heads, self.head_dimension).transpose(1, 2)\n",
    "                             for net, x in zip(self.qkv_nets, (query, key, value))]\n",
    "\n",
    "        # Step 2: Apply attention - compare query with key and use that to combine values (see the function for details)\n",
    "        intermediate_token_representations, attention_weights = self.attention(query, key, value, mask)\n",
    "\n",
    "        # Potentially, for visualization purposes, log the attention weights, turn off during training though!\n",
    "        # I had memory problems when I leave this on by default\n",
    "        if self.log_attention_weights:\n",
    "            self.attention_weights = attention_weights\n",
    "\n",
    "        # Step 3: Reshape from (B, NH, S/T, HD) over (B, S/T, NH, HD) (via transpose) into (B, S/T, NHxHD) which is\n",
    "        # the same shape as in the beginning of this forward function i.e. input to MHA (multi-head attention) module\n",
    "        reshaped = intermediate_token_representations.transpose(1, 2).reshape(batch_size, -1, self.number_of_heads * self.head_dimension)\n",
    "\n",
    "        # Step 4: Output linear projection\n",
    "        token_representations = self.out_projection_net(reshaped)\n",
    "\n",
    "        return token_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 200, 3])\n"
     ]
    }
   ],
   "source": [
    "# !pip install positional-encodings\n",
    "from positional_encodings.torch_encodings import PositionalEncoding2D\n",
    "\n",
    "p_enc_2d = PositionalEncoding2D(3)\n",
    "y = torch.zeros((1,200,200,3))\n",
    "print(p_enc_2d(y).shape) # (1, 6, 2, 8)\n",
    "yy = p_enc_2d(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(yy[0,:5,:5]); plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy[0,:5,:5]\n",
    "# generate pos embeddings offline\n",
    "\n",
    "pos_dim = 10\n",
    "\n",
    "p_enc_2d = PositionalEncoding2D(pos_dim)\n",
    "pos_embeddings = {}\n",
    "# for i in range(1,200):\n",
    "#     for j in range(2,100):    \n",
    "z = torch.zeros(1,300,300,pos_dim)\n",
    "# pos_embeddings[(i,j)] = p_enc_2d(z[:,:i,:j,:].to(DEVICE))\n",
    "pos_embeddings = p_enc_2d(z.to(DEVICE))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300, 300, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention_KV(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, number_of_heads, dropout_probability, log_attention_weights):\n",
    "        super().__init__()\n",
    "        assert model_dimension % number_of_heads == 0, f'Model dimension must be divisible by the number of heads.'\n",
    "\n",
    "        self.head_dimension = int(model_dimension / number_of_heads)\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        self.kv_nets = get_clones(nn.Linear(model_dimension, model_dimension), 3)  # identity activation hence \"nets\"\n",
    "        \n",
    "        # self.kk = nn.Linear(2*self.head_dimension, 1, bias=True)          \n",
    "        self.map_pos = nn.Linear(pos_dim, 1, bias=True)          \n",
    "        \n",
    "        self.out_projection_net = nn.Linear(model_dimension, model_dimension)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(p=dropout_probability)  # no pun intended, not explicitly mentioned in paper\n",
    "        self.softmax = nn.Softmax(dim=-1)  # -1 stands for apply the softmax along the last dimension\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # should we log attention weights\n",
    "        self.attention_weights = None  # for visualization purposes, I cache the weights here (translation_script.py)\n",
    "        \n",
    "        \n",
    "    def attention(self, query, key, value, mask):\n",
    "        \n",
    "        b, h, n, _ = query.shape\n",
    "        b, h, m, _ = key.shape\n",
    "        \n",
    "        tmp = key if n==m else query\n",
    "        scores = torch.matmul(tmp, key.transpose(-2, -1)) / math.sqrt(self.head_dimension)\n",
    "\n",
    "        if attn_type == 'kv':\n",
    "            pos = pos_embeddings[:,:n,:m,:] # 1, m, m, ? \n",
    "            scores = scores.unsqueeze(-1) + pos.unsqueeze(0)\n",
    "            scores = self.map_pos(scores).squeeze(-1)\n",
    "\n",
    "        \n",
    "        # Step 2: Optionally mask tokens whose representations we want to ignore by setting a big negative number\n",
    "        # to locations corresponding to those tokens (force softmax to output 0 probability on those locations).\n",
    "        # mask shape = (B, 1, 1, S) or (B, 1, T, T) will get broad-casted (copied) as needed to match scores shape\n",
    "\n",
    "        # if mask is not None:\n",
    "        scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n",
    "\n",
    "        # Step 3: Calculate the attention weights - how much should we attend to surrounding token representations\n",
    "        attention_weights = self.softmax(scores)\n",
    "        \n",
    "        # Step 4: Not defined in the original paper apply dropout to attention weights as well\n",
    "        attention_weights = self.attention_dropout(attention_weights)\n",
    "\n",
    "        # Step 5: based on attention weights calculate new token representations\n",
    "        # attention_weights shape = (B, NH, S, S)/(B, NH, T, T) or (B, NH, T, S), value shape = (B, NH, S/T, HD)\n",
    "        # Final shape (B, NH, S, HD) for source MHAs or (B, NH, T, HD) target MHAs (again MHAs are used in 3 contexts)\n",
    "        intermediate_token_representations = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return intermediate_token_representations, attention_weights  # attention weights for visualization purposes\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Step 1: Input linear projection\n",
    "        # Notation: B - batch size, NH - number of heads, S/T - max src/trg token-sequence length, HD - head dimension\n",
    "        # Shape goes from (B, S/T, NH*HD) over (B, S/T, NH, HD) to (B, NH, S/T, HD) (NH*HD=D where D is model dimension)\n",
    "        query, key, value = [net(x).view(batch_size, -1, self.number_of_heads, self.head_dimension).transpose(1, 2)\n",
    "                             for net, x in zip([self.kv_nets[0], self.kv_nets[1], self.kv_nets[2]],  (query, key, value))]\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Step 2: Apply attention - compare query with key and use that to combine values (see the function for details)\n",
    "        intermediate_token_representations, attention_weights = self.attention(query, key, value, mask)\n",
    "\n",
    "        # Potentially, for visualization purposes, log the attention weights, turn off during training though!\n",
    "        # I had memory problems when I leave this on by default\n",
    "        if self.log_attention_weights:\n",
    "            self.attention_weights = attention_weights\n",
    "\n",
    "        # Step 3: Reshape from (B, NH, S/T, HD) over (B, S/T, NH, HD) (via transpose) into (B, S/T, NHxHD) which is\n",
    "        # the same shape as in the beginning of this forward function i.e. input to MHA (multi-head attention) module\n",
    "        reshaped = intermediate_token_representations.transpose(1, 2).reshape(batch_size, -1, self.number_of_heads * self.head_dimension)\n",
    "\n",
    "        # Step 4: Output linear projection\n",
    "        token_representations = self.out_projection_net(reshaped)\n",
    "\n",
    "        return token_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the BLEU-4 score\n",
    "def calculate_bleu_score(transformer, token_ids_loader, trg_field_processor):\n",
    "    with torch.no_grad():\n",
    "        pad_token_id = PAD_IDX #trg_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "\n",
    "        gt_sentences_corpus = []\n",
    "        predicted_sentences_corpus = []\n",
    "\n",
    "        ts = time.time()\n",
    "        for batch_idx, token_ids_batch in enumerate(token_ids_loader):\n",
    "            # if batch_idx > 30: break # ali!!!\n",
    "            \n",
    "            # import pdb; pdb.set_trace()\n",
    "            # src_token_ids_batch, trg_token_ids_batch = token_ids_batch.src, token_ids_batch.trg\n",
    "            src_token_ids_batch, trg_token_ids_batch = token_ids_batch#.src, token_ids_batch.trg\n",
    "            src_token_ids_batch, trg_token_ids_batch =  src_token_ids_batch.to(DEVICE), trg_token_ids_batch.to(DEVICE)\n",
    "            src_token_ids_batch, trg_token_ids_batch = src_token_ids_batch.transpose(0,1), trg_token_ids_batch.transpose(0,1)              \n",
    "            \n",
    "            \n",
    "            # if batch_idx % 10 == 0:\n",
    "            #     print(f'batch={batch_idx}, time elapsed = {time.time()-ts} seconds.')\n",
    "\n",
    "            # Optimization - compute the source token representations only once\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # try:\n",
    "            src_token_ids_batch = src_token_ids_batch.to(dtype=torch.int64)\n",
    "            src_mask, _ = get_masks_and_count_tokens_src(src_token_ids_batch, pad_token_id)\n",
    "            src_representations_batch = transformer.encode(src_token_ids_batch, src_mask)\n",
    "            # except Exception as e:\n",
    "            # print(src_token_ids_batch)\n",
    "            # print(e)\n",
    "\n",
    "            predicted_sentences = greedy_decoding(transformer, src_representations_batch, src_mask, trg_field_processor)\n",
    "            predicted_sentences_corpus.extend(predicted_sentences)  # add them to the corpus of translations\n",
    "\n",
    "            # Get the token and not id version of GT (ground-truth) sentences\n",
    "            trg_token_ids_batch = trg_token_ids_batch.cpu().numpy()\n",
    "            # for target_sentence_ids in trg_token_ids_batch:\n",
    "            #     target_sentence_tokens = [trg_field_processor.vocab.itos[id] for id in target_sentence_ids if id != pad_token_id]\n",
    "            #     gt_sentences_corpus.append([target_sentence_tokens])  # add them to the corpus of GT translations\n",
    "            for target_sentence_ids in trg_token_ids_batch:\n",
    "                target_sentence_tokens = [trg_field_processor.lookup_tokens([id])[0] for id in target_sentence_ids if id != pad_token_id]\n",
    "                gt_sentences_corpus.append(target_sentence_tokens)  # add them to the corpus of GT translations\n",
    "        \n",
    "        try:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # hh = [i[0] for i in gt_sentences_corpus]\n",
    "            hh = [[i] for i in gt_sentences_corpus]\n",
    "            bleu_score = corpus_bleu(hh, predicted_sentences_corpus)\n",
    "            # bleu_score = corpus_bleu(gt_sentences_corpus, predicted_sentences_corpus)\n",
    "            print(f'BLEU-4 corpus score = {bleu_score}, corpus length = {len(gt_sentences_corpus)}, time elapsed = {time.time()-ts} seconds.')\n",
    "        except:\n",
    "            print('ERROR in BLEU!!')\n",
    "            bleu_score = 0\n",
    "        return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(baseline_transformer, src_representations_batch, src_mask, trg_field_processor, max_target_tokens=100):\n",
    "    \"\"\"\n",
    "    Supports batch (decode multiple source sentences) greedy decoding.\n",
    "\n",
    "    Decoding could be further optimized to cache old token activations because they can't look ahead and so\n",
    "    adding a newly predicted token won't change old token's activations.\n",
    "\n",
    "    Example: we input <s> and do a forward pass. We get intermediate activations for <s> and at the output at position\n",
    "    0, after the doing linear layer we get e.g. token <I>. Now we input <s>,<I> but <s>'s activations will remain\n",
    "    the same. Similarly say we now got <am> at output position 1, in the next step we input <s>,<I>,<am> and so <I>'s\n",
    "    activations will remain the same as it only looks at/attends to itself and to <s> and so forth.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    device = next(baseline_transformer.parameters()).device\n",
    "    # pad_token_id = trg_field_processor.vocab.stoi[PAD_TOKEN]\n",
    "    pad_token_id = PAD_IDX\n",
    "\n",
    "    \n",
    "    \n",
    "    # BOS_IDX = BOS_TOKEN\n",
    "    target_sentences_tokens = [[BOS_TOKEN] for _ in range(src_representations_batch.shape[0])]\n",
    "    # trg_token_ids_batch = torch.tensor([[trg_field_processor.vocab.stoi[tokens[0]]] for tokens in target_sentences_tokens], device=device)\n",
    "    \n",
    "    trg_token_ids_batch = torch.tensor([[trg_field_processor[tokens[0]]] for tokens in target_sentences_tokens], device=device)    \n",
    "\n",
    "    # Set to true for a particular target sentence once it reaches the EOS (end-of-sentence) token\n",
    "    is_decoded = [False] * src_representations_batch.shape[0]\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    while True:\n",
    "        trg_mask, _ = get_masks_and_count_tokens_trg(trg_token_ids_batch, pad_token_id)\n",
    "        # Shape = (B*T, V) where T is the current token-sequence length and V target vocab size\n",
    "        predicted_log_distributions = baseline_transformer.decode(trg_token_ids_batch, src_representations_batch, trg_mask, src_mask)\n",
    "\n",
    "        # Extract only the indices of last token for every target sentence (we take every T-th token)\n",
    "        num_of_trg_tokens = len(target_sentences_tokens[0])\n",
    "        predicted_log_distributions = predicted_log_distributions[num_of_trg_tokens-1::num_of_trg_tokens]\n",
    "\n",
    "        # This is the \"greedy\" part of the greedy decoding:\n",
    "        # We find indices of the highest probability target tokens and discard every other possibility\n",
    "        most_probable_last_token_indices = torch.argmax(predicted_log_distributions, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Find target tokens associated with these indices\n",
    "        # predicted_words = [trg_field_processor.vocab.itos[index] for index in most_probable_last_token_indices]\n",
    "        predicted_words = [trg_field_processor.lookup_tokens([index])[0] for index in most_probable_last_token_indices]\n",
    "        \n",
    "        for idx, predicted_word in enumerate(predicted_words):\n",
    "            target_sentences_tokens[idx].append(predicted_word)\n",
    "\n",
    "            if predicted_word == EOS_TOKEN:  # once we find EOS token for a particular sentence we flag it\n",
    "                is_decoded[idx] = True\n",
    "\n",
    "        if all(is_decoded) or num_of_trg_tokens == max_target_tokens:\n",
    "            break\n",
    "\n",
    "        # Prepare the input for the next iteration (merge old token ids with the new column of most probable token ids)\n",
    "        trg_token_ids_batch = torch.cat((trg_token_ids_batch, torch.unsqueeze(torch.tensor(most_probable_last_token_indices, device=device), 1)), 1)\n",
    "\n",
    "    # Post process the sentences - remove everything after the EOS token\n",
    "    target_sentences_tokens_post = []\n",
    "    for target_sentence_tokens in target_sentences_tokens:\n",
    "        try:\n",
    "            target_index = target_sentence_tokens.index(EOS_TOKEN) + 1\n",
    "        except:\n",
    "            target_index = None\n",
    "\n",
    "        target_sentence_tokens = target_sentence_tokens[:target_index]\n",
    "        target_sentences_tokens_post.append(target_sentence_tokens)\n",
    "\n",
    "    return target_sentences_tokens_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu_score = calculate_bleu_score(transformer, test_dataloader, vocab_transform[TGT_LANGUAGE])\n",
    "# print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F7hDRk_SPjj"
   },
   "source": [
    "During training, we need a subsequent word mask that will prevent the model from looking into\n",
    "the future words when making predictions. We will also need masks to hide\n",
    "source and target padding tokens. Below, let's define a function that will take care of both.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zklLlqvmSPjk"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX)#.transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX)#.transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Instead of one-hot target distribution set the target word's probability to \"confidence_value\" (usually 0.9)\n",
    "        and distribute the rest of the \"smoothing_value\" mass (usually 0.1) over the rest of the vocab.\n",
    "\n",
    "        Check out playground.py for visualization of how the smooth target distribution looks like compared to one-hot.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smoothing_value, pad_token_id, trg_vocab_size, device):\n",
    "        assert 0.0 <= smoothing_value <= 1.0\n",
    "\n",
    "        super(LabelSmoothingDistribution, self).__init__()\n",
    "\n",
    "        self.confidence_value = 1.0 - smoothing_value\n",
    "        self.smoothing_value = smoothing_value\n",
    "\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, trg_token_ids_batch):\n",
    "\n",
    "        batch_size = trg_token_ids_batch.shape[0]\n",
    "        smooth_target_distributions = torch.zeros((batch_size, self.trg_vocab_size), device=self.device)\n",
    "\n",
    "        # -2 because we are not distributing the smoothing mass over the pad token index and over the ground truth index\n",
    "        # those 2 values will be overwritten by the following 2 lines with confidence_value and 0 (for pad token index)\n",
    "        smooth_target_distributions.fill_(self.smoothing_value / (self.trg_vocab_size - 2))\n",
    "\n",
    "        smooth_target_distributions.scatter_(1, trg_token_ids_batch, self.confidence_value)\n",
    "        smooth_target_distributions[:, self.pad_token_id] = 0.\n",
    "\n",
    "        # If we had a pad token as a target we set the distribution to all 0s instead of smooth labeled distribution\n",
    "        smooth_target_distributions.masked_fill_(trg_token_ids_batch == self.pad_token_id, 0.)\n",
    "\n",
    "        return smooth_target_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mnvw3_cqSPjl"
   },
   "source": [
    "Let's now define the parameters of our model and instantiate the same. Below, we also\n",
    "define our loss function which is the cross-entropy loss and the optimizer used for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CATrmm3ASPjm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRC_VOCAB_SIZE\n",
    "# vocab_transform[SRC_LANGUAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5MYAwkjSPjn"
   },
   "source": [
    "## Collation\n",
    "\n",
    "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings.\n",
    "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network\n",
    "defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that\n",
    "can be fed directly into our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "oINq18RbSPjn"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_and_count_tokens_src(src_token_ids_batch, pad_token_id):\n",
    "    batch_size = src_token_ids_batch.shape[0]\n",
    "\n",
    "    # src_mask shape = (B, 1, 1, S) check out attention function in transformer_model.py where masks are applied\n",
    "    # src_mask only masks pad tokens as we want to ignore their representations (no information in there...)\n",
    "    src_mask = (src_token_ids_batch != pad_token_id).view(batch_size, 1, 1, -1)\n",
    "    num_src_tokens = torch.sum(src_mask.long())\n",
    "\n",
    "    return src_mask, num_src_tokens\n",
    "\n",
    "\n",
    "def get_masks_and_count_tokens_trg(trg_token_ids_batch, pad_token_id):\n",
    "    batch_size = trg_token_ids_batch.shape[0]\n",
    "    device = trg_token_ids_batch.device\n",
    "\n",
    "    # Same as src_mask but we additionally want to mask tokens from looking forward into the future tokens\n",
    "    # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
    "    sequence_length = trg_token_ids_batch.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
    "    trg_padding_mask = (trg_token_ids_batch != pad_token_id).view(batch_size, 1, 1, -1)  # shape = (B, 1, 1, T)\n",
    "    trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
    "\n",
    "    # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
    "    trg_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
    "    num_trg_tokens = torch.sum(trg_padding_mask.long())\n",
    "\n",
    "    return trg_mask, num_trg_tokens\n",
    "\n",
    "\n",
    "def get_masks_and_count_tokens(src_token_ids_batch, trg_token_ids_batch, pad_token_id, device):\n",
    "    src_mask, num_src_tokens = get_masks_and_count_tokens_src(src_token_ids_batch, pad_token_id)\n",
    "    trg_mask, num_trg_tokens = get_masks_and_count_tokens_trg(trg_token_ids_batch, pad_token_id)\n",
    "\n",
    "    return src_mask, trg_mask, num_src_tokens, num_trg_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXLfMkF_SPjo"
   },
   "source": [
    "Let's define training and evaluation loop that will be called for each\n",
    "epoch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ql9leGK0SPjp"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn,  shuffle = True)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        src = src.transpose(0,1)\n",
    "        tgt = tgt.transpose(0,1)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        # src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        src_mask, tgt_mask, _, _ = get_masks_and_count_tokens(src, tgt_input, PAD_IDX, DEVICE)                    \n",
    "\n",
    "        # logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        src = src.to(dtype=torch.int64)\n",
    "        tgt_input = tgt_input.to(dtype=torch.int64)\n",
    "        \n",
    "        # try:\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        # except:\n",
    "        #     import pdb; pdb.set_trace()\n",
    "            \n",
    "        # logits = model(src, tgt_input, src_padding_mask, tgt_padding_mask)\n",
    "        # predicted_log_distributions = baseline_transformer(src_token_ids_batch, trg_token_ids_batch_input, src_mask, trg_mask)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # tgt_out = tgt[1:, :]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        # loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        \n",
    "        trg_token_ids_batch_gt = tgt[:, 1:].reshape(-1, 1)\n",
    "        trg_token_ids_batch_gt = trg_token_ids_batch_gt.to(dtype=torch.int64)\n",
    "        smooth_target_distributions = label_smoothing(trg_token_ids_batch_gt)  # these are regular probabilities\n",
    "        loss = kl_div_loss(logits, smooth_target_distributions)\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer.step()\n",
    "        custom_lr_optimizer.step()\n",
    "        \n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        src = src.transpose(0,1)\n",
    "        tgt = tgt.transpose(0,1)\n",
    "        \n",
    "        \n",
    "        # tgt_input = tgt[:-1, :]\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, _, _ = get_masks_and_count_tokens(src, tgt_input, PAD_IDX, DEVICE)                    \n",
    "        \n",
    "        src = src.to(dtype=torch.int64)\n",
    "        tgt_input = tgt_input.to(dtype=torch.int64)\n",
    "        \n",
    "        try:\n",
    "            logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        except:\n",
    "            import pdb; pdb.set_trace()\n",
    "        \n",
    "        # src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        # logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        # tgt_out = tgt[1:, :]\n",
    "        tgt_out = tgt[:,1:]\n",
    "        # loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        trg_token_ids_batch_gt = tgt[:, 1:].reshape(-1, 1)\n",
    "        trg_token_ids_batch_gt = trg_token_ids_batch_gt.to(dtype=torch.int64)\n",
    "        smooth_target_distributions = label_smoothing(trg_token_ids_batch_gt)  # these are regular probabilities\n",
    "        loss = kl_div_loss(logits, smooth_target_distributions)\n",
    "        \n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRAdamOptimizer:\n",
    "    \"\"\"\n",
    "        Linear ramp learning rate for the warm-up number of steps and then start decaying\n",
    "        according to the inverse square root law of the current training step number.\n",
    "\n",
    "        Check out playground.py for visualization of the learning rate (visualize_custom_lr_adam).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, model_dimension, num_of_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_size = model_dimension\n",
    "        self.num_of_warmup_steps = num_of_warmup_steps\n",
    "\n",
    "        self.current_step_number = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step_number += 1\n",
    "        current_learning_rate = self.get_current_learning_rate()\n",
    "\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = current_learning_rate\n",
    "\n",
    "        self.optimizer.step()  # apply gradients\n",
    "\n",
    "    # Check out the formula at Page 7, Chapter 5.3 \"Optimizer\" and playground.py for visualization\n",
    "    def get_current_learning_rate(self):\n",
    "        # For readability purpose\n",
    "        step = self.current_step_number\n",
    "        warmup = self.num_of_warmup_steps\n",
    "\n",
    "        return self.model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "\n",
    "# attn_type = 'kv'\n",
    "num_runs = 2\n",
    "which_task = f'translation_{SRC_LANGUAGE}_{TGT_LANGUAGE}'\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "def define_model():\n",
    "    transformer = Transformer(\n",
    "            model_dimension=BASELINE_MODEL_DIMENSION,\n",
    "            src_vocab_size=SRC_VOCAB_SIZE,\n",
    "            trg_vocab_size=TGT_VOCAB_SIZE,\n",
    "            number_of_heads=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "            number_of_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "            dropout_probability=BASELINE_MODEL_DROPOUT_PROB\n",
    "        ).to(DEVICE)\n",
    "\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # transformer = transformer.to(DEVICE)\n",
    "\n",
    "\n",
    "    # loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    kl_div_loss = nn.KLDivLoss(reduction='batchmean')  # gives better BLEU score than \"mean\"\n",
    "    # optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # for KV\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "    custom_lr_optimizer = CustomLRAdamOptimizer(\n",
    "                Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-9),\n",
    "                BASELINE_MODEL_DIMENSION,\n",
    "                4000 # num warmup steps\n",
    "            )\n",
    "    \n",
    "    return transformer, kl_div_loss, optimizer, custom_lr_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name\n",
    "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "BASELINE_MODEL_DROPOUT_PROB = 0.1\n",
    "\n",
    "BASELINE_MODEL_LABEL_SMOOTHING_VALUE = 0.1\n",
    "\n",
    "label_smoothing = LabelSmoothingDistribution(BASELINE_MODEL_LABEL_SMOOTHING_VALUE, PAD_IDX, TGT_VOCAB_SIZE, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "import copy\n",
    "from torch.optim import Adam\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation_en_de---------- norm type: kv, run number: 0 ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.680, Val loss: 2.772, Epoch time = 38.194s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/miniconda3/envs/py39/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ali/miniconda3/envs/py39/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 corpus score = 1.6010157907937408e-156, corpus length = 1000, time elapsed = 0.9775991439819336 seconds.\n",
      "Epoch: 2, Train loss: 2.234, Val loss: 1.988, Epoch time = 75.060s\n",
      "BLEU-4 corpus score = 0.06996482449497146, corpus length = 1000, time elapsed = 8.511854410171509 seconds.\n",
      "Epoch: 3, Train loss: 1.810, Val loss: 1.705, Epoch time = 120.062s\n",
      "BLEU-4 corpus score = 0.14205408466755146, corpus length = 1000, time elapsed = 6.2505576610565186 seconds.\n",
      "Epoch: 4, Train loss: 1.598, Val loss: 1.537, Epoch time = 163.027s\n",
      "BLEU-4 corpus score = 0.17695563116886215, corpus length = 1000, time elapsed = 8.70157766342163 seconds.\n",
      "Epoch: 5, Train loss: 1.454, Val loss: 1.422, Epoch time = 208.162s\n",
      "BLEU-4 corpus score = 0.23377143638444187, corpus length = 1000, time elapsed = 5.5308518409729 seconds.\n",
      "Epoch: 6, Train loss: 1.345, Val loss: 1.358, Epoch time = 250.213s\n",
      "BLEU-4 corpus score = 0.22666851966910048, corpus length = 1000, time elapsed = 12.768577098846436 seconds.\n",
      "Epoch: 7, Train loss: 1.255, Val loss: 1.316, Epoch time = 299.609s\n",
      "BLEU-4 corpus score = 0.26553091386269845, corpus length = 1000, time elapsed = 5.5928192138671875 seconds.\n",
      "Epoch: 8, Train loss: 1.191, Val loss: 1.275, Epoch time = 342.250s\n",
      "BLEU-4 corpus score = 0.28112503062755306, corpus length = 1000, time elapsed = 4.350905179977417 seconds.\n",
      "Epoch: 9, Train loss: 1.139, Val loss: 1.249, Epoch time = 383.407s\n",
      "BLEU-4 corpus score = 0.29194270244123627, corpus length = 1000, time elapsed = 4.877958059310913 seconds.\n",
      "Epoch: 10, Train loss: 1.089, Val loss: 1.229, Epoch time = 425.377s\n",
      "BLEU-4 corpus score = 0.28306969436111423, corpus length = 1000, time elapsed = 8.393510103225708 seconds.\n",
      "Epoch: 11, Train loss: 1.057, Val loss: 1.225, Epoch time = 470.523s\n",
      "BLEU-4 corpus score = 0.30289682775107873, corpus length = 1000, time elapsed = 6.332963228225708 seconds.\n",
      "Epoch: 12, Train loss: 1.018, Val loss: 1.200, Epoch time = 513.727s\n",
      "BLEU-4 corpus score = 0.3002791024735249, corpus length = 1000, time elapsed = 5.696190118789673 seconds.\n",
      "Epoch: 13, Train loss: 0.999, Val loss: 1.192, Epoch time = 556.177s\n",
      "BLEU-4 corpus score = 0.2860238653535597, corpus length = 1000, time elapsed = 6.424878835678101 seconds.\n",
      "Epoch: 14, Train loss: 0.966, Val loss: 1.202, Epoch time = 599.317s\n",
      "BLEU-4 corpus score = 0.30882533732302336, corpus length = 1000, time elapsed = 6.298138618469238 seconds.\n",
      "Epoch: 15, Train loss: 0.957, Val loss: 1.183, Epoch time = 642.386s\n",
      "BLEU-4 corpus score = 0.3024353868098629, corpus length = 1000, time elapsed = 7.3173439502716064 seconds.\n",
      "Epoch: 16, Train loss: 0.941, Val loss: 1.180, Epoch time = 686.163s\n",
      "BLEU-4 corpus score = 0.2998191282219447, corpus length = 1000, time elapsed = 8.698014497756958 seconds.\n",
      "Epoch: 17, Train loss: 0.919, Val loss: 1.175, Epoch time = 731.366s\n",
      "BLEU-4 corpus score = 0.32134714317187146, corpus length = 1000, time elapsed = 4.091968297958374 seconds.\n",
      "Epoch: 18, Train loss: 0.911, Val loss: 1.176, Epoch time = 772.458s\n",
      "BLEU-4 corpus score = 0.3030229738597911, corpus length = 1000, time elapsed = 8.463731288909912 seconds.\n",
      "Epoch: 19, Train loss: 0.894, Val loss: 1.176, Epoch time = 817.569s\n",
      "BLEU-4 corpus score = 0.3115436753460414, corpus length = 1000, time elapsed = 6.4344801902771 seconds.\n",
      "Epoch: 20, Train loss: 0.890, Val loss: 1.168, Epoch time = 859.950s\n",
      "BLEU-4 corpus score = 0.3029106938981516, corpus length = 1000, time elapsed = 6.6000940799713135 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.698, Val loss: 2.796, Epoch time = 36.273s\n",
      "BLEU-4 corpus score = 2.067804235311808e-157, corpus length = 1000, time elapsed = 0.7428791522979736 seconds.\n",
      "Epoch: 2, Train loss: 2.274, Val loss: 2.037, Epoch time = 73.101s\n",
      "BLEU-4 corpus score = 0.06795554074070674, corpus length = 1000, time elapsed = 19.083527326583862 seconds.\n",
      "Epoch: 3, Train loss: 1.842, Val loss: 1.763, Epoch time = 128.891s\n",
      "BLEU-4 corpus score = 0.10403968372735087, corpus length = 1000, time elapsed = 10.96144986152649 seconds.\n",
      "Epoch: 4, Train loss: 1.628, Val loss: 1.571, Epoch time = 176.728s\n",
      "BLEU-4 corpus score = 0.17191308857264745, corpus length = 1000, time elapsed = 6.008214235305786 seconds.\n",
      "Epoch: 5, Train loss: 1.464, Val loss: 1.430, Epoch time = 219.170s\n",
      "BLEU-4 corpus score = 0.21952789101750128, corpus length = 1000, time elapsed = 6.058027505874634 seconds.\n",
      "Epoch: 6, Train loss: 1.345, Val loss: 1.356, Epoch time = 261.887s\n",
      "BLEU-4 corpus score = 0.2588703487272644, corpus length = 1000, time elapsed = 6.645938873291016 seconds.\n",
      "Epoch: 7, Train loss: 1.267, Val loss: 1.327, Epoch time = 305.589s\n",
      "BLEU-4 corpus score = 0.2651424494885112, corpus length = 1000, time elapsed = 4.909807443618774 seconds.\n",
      "Epoch: 8, Train loss: 1.188, Val loss: 1.269, Epoch time = 346.926s\n",
      "BLEU-4 corpus score = 0.2795125009616795, corpus length = 1000, time elapsed = 9.484258651733398 seconds.\n",
      "Epoch: 9, Train loss: 1.131, Val loss: 1.240, Epoch time = 392.962s\n",
      "BLEU-4 corpus score = 0.29859962612929725, corpus length = 1000, time elapsed = 5.150161981582642 seconds.\n",
      "Epoch: 10, Train loss: 1.086, Val loss: 1.218, Epoch time = 435.379s\n",
      "BLEU-4 corpus score = 0.3012668142349866, corpus length = 1000, time elapsed = 3.8827171325683594 seconds.\n",
      "Epoch: 11, Train loss: 1.043, Val loss: 1.211, Epoch time = 475.979s\n",
      "BLEU-4 corpus score = 0.30875529434063265, corpus length = 1000, time elapsed = 3.7550597190856934 seconds.\n",
      "Epoch: 12, Train loss: 1.013, Val loss: 1.201, Epoch time = 516.121s\n",
      "BLEU-4 corpus score = 0.3153314983608514, corpus length = 1000, time elapsed = 3.5228168964385986 seconds.\n",
      "Epoch: 13, Train loss: 0.991, Val loss: 1.186, Epoch time = 555.663s\n",
      "BLEU-4 corpus score = 0.3043516102291444, corpus length = 1000, time elapsed = 5.222637176513672 seconds.\n",
      "Epoch: 14, Train loss: 0.964, Val loss: 1.183, Epoch time = 596.873s\n",
      "BLEU-4 corpus score = 0.3149152486472847, corpus length = 1000, time elapsed = 4.7074666023254395 seconds.\n",
      "Epoch: 15, Train loss: 0.944, Val loss: 1.180, Epoch time = 637.608s\n",
      "BLEU-4 corpus score = 0.3092143934853099, corpus length = 1000, time elapsed = 5.546941518783569 seconds.\n",
      "Epoch: 16, Train loss: 0.930, Val loss: 1.176, Epoch time = 679.158s\n",
      "BLEU-4 corpus score = 0.3153869281042195, corpus length = 1000, time elapsed = 3.6437761783599854 seconds.\n",
      "Epoch: 17, Train loss: 0.912, Val loss: 1.181, Epoch time = 718.846s\n",
      "BLEU-4 corpus score = 0.3178257799863188, corpus length = 1000, time elapsed = 4.102612733840942 seconds.\n",
      "Epoch: 18, Train loss: 0.907, Val loss: 1.169, Epoch time = 758.986s\n",
      "BLEU-4 corpus score = 0.30421320649354433, corpus length = 1000, time elapsed = 6.683306694030762 seconds.\n",
      "Epoch: 19, Train loss: 0.889, Val loss: 1.172, Epoch time = 801.638s\n",
      "BLEU-4 corpus score = 0.3048130647049466, corpus length = 1000, time elapsed = 5.14557957649231 seconds.\n",
      "Epoch: 20, Train loss: 0.876, Val loss: 1.176, Epoch time = 842.775s\n",
      "BLEU-4 corpus score = 0.3160584938318699, corpus length = 1000, time elapsed = 5.004319906234741 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.664, Val loss: 2.766, Epoch time = 34.764s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/miniconda3/envs/py39/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 corpus score = 4.2849027717056804e-234, corpus length = 1000, time elapsed = 0.6220293045043945 seconds.\n",
      "Epoch: 2, Train loss: 2.240, Val loss: 1.981, Epoch time = 70.162s\n",
      "BLEU-4 corpus score = 0.04408060017866504, corpus length = 1000, time elapsed = 19.518733024597168 seconds.\n",
      "Epoch: 3, Train loss: 1.786, Val loss: 1.676, Epoch time = 124.469s\n",
      "BLEU-4 corpus score = 0.16496876520031528, corpus length = 1000, time elapsed = 3.7280468940734863 seconds.\n",
      "Epoch: 4, Train loss: 1.544, Val loss: 1.474, Epoch time = 163.001s\n",
      "BLEU-4 corpus score = 0.21923017838479877, corpus length = 1000, time elapsed = 10.401109218597412 seconds.\n",
      "Epoch: 5, Train loss: 1.366, Val loss: 1.351, Epoch time = 208.205s\n",
      "BLEU-4 corpus score = 0.28971574671657974, corpus length = 1000, time elapsed = 3.6849050521850586 seconds.\n",
      "Epoch: 6, Train loss: 1.258, Val loss: 1.274, Epoch time = 246.623s\n",
      "BLEU-4 corpus score = 0.2886067811366814, corpus length = 1000, time elapsed = 7.9016032218933105 seconds.\n",
      "Epoch: 7, Train loss: 1.172, Val loss: 1.226, Epoch time = 289.277s\n",
      "BLEU-4 corpus score = 0.31257392864919964, corpus length = 1000, time elapsed = 7.0992701053619385 seconds.\n",
      "Epoch: 8, Train loss: 1.110, Val loss: 1.196, Epoch time = 331.114s\n",
      "BLEU-4 corpus score = 0.33925507886847983, corpus length = 1000, time elapsed = 4.778005599975586 seconds.\n",
      "Epoch: 9, Train loss: 1.040, Val loss: 1.166, Epoch time = 370.632s\n",
      "BLEU-4 corpus score = 0.33567281207733995, corpus length = 1000, time elapsed = 5.847419023513794 seconds.\n",
      "Epoch: 10, Train loss: 1.001, Val loss: 1.144, Epoch time = 411.204s\n",
      "BLEU-4 corpus score = 0.33773936892214884, corpus length = 1000, time elapsed = 3.544807195663452 seconds.\n",
      "Epoch: 11, Train loss: 0.960, Val loss: 1.134, Epoch time = 449.541s\n",
      "BLEU-4 corpus score = 0.3472437370963457, corpus length = 1000, time elapsed = 3.3104865550994873 seconds.\n",
      "Epoch: 12, Train loss: 0.930, Val loss: 1.123, Epoch time = 487.618s\n",
      "BLEU-4 corpus score = 0.3511625953710262, corpus length = 1000, time elapsed = 3.37129807472229 seconds.\n",
      "Epoch: 13, Train loss: 0.911, Val loss: 1.114, Epoch time = 525.726s\n",
      "BLEU-4 corpus score = 0.3478629841869991, corpus length = 1000, time elapsed = 4.59132194519043 seconds.\n",
      "Epoch: 14, Train loss: 0.886, Val loss: 1.108, Epoch time = 565.359s\n",
      "BLEU-4 corpus score = 0.34337071742117986, corpus length = 1000, time elapsed = 6.792182683944702 seconds.\n",
      "Epoch: 15, Train loss: 0.865, Val loss: 1.106, Epoch time = 607.603s\n",
      "BLEU-4 corpus score = 0.3590099626973152, corpus length = 1000, time elapsed = 3.673257827758789 seconds.\n",
      "Epoch: 16, Train loss: 0.859, Val loss: 1.099, Epoch time = 646.382s\n",
      "BLEU-4 corpus score = 0.3482073197741767, corpus length = 1000, time elapsed = 4.385868787765503 seconds.\n",
      "Epoch: 17, Train loss: 0.840, Val loss: 1.091, Epoch time = 685.641s\n",
      "BLEU-4 corpus score = 0.35506039180365395, corpus length = 1000, time elapsed = 4.338951349258423 seconds.\n",
      "Epoch: 18, Train loss: 0.818, Val loss: 1.095, Epoch time = 724.970s\n",
      "BLEU-4 corpus score = 0.3419297124632688, corpus length = 1000, time elapsed = 5.389331340789795 seconds.\n",
      "Epoch: 19, Train loss: 0.817, Val loss: 1.102, Epoch time = 765.353s\n",
      "BLEU-4 corpus score = 0.35706655928246966, corpus length = 1000, time elapsed = 4.271242141723633 seconds.\n",
      "Epoch: 20, Train loss: 0.801, Val loss: 1.095, Epoch time = 804.641s\n",
      "BLEU-4 corpus score = 0.3478221569334091, corpus length = 1000, time elapsed = 5.1245574951171875 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.674, Val loss: 2.766, Epoch time = 34.859s\n",
      "BLEU-4 corpus score = 4.2849027717056804e-234, corpus length = 1000, time elapsed = 0.6203927993774414 seconds.\n",
      "Epoch: 2, Train loss: 2.232, Val loss: 1.980, Epoch time = 70.411s\n",
      "BLEU-4 corpus score = 0.08201522577157938, corpus length = 1000, time elapsed = 9.326817989349365 seconds.\n",
      "Epoch: 3, Train loss: 1.790, Val loss: 1.671, Epoch time = 114.940s\n",
      "BLEU-4 corpus score = 0.15588285266598093, corpus length = 1000, time elapsed = 7.3664727210998535 seconds.\n",
      "Epoch: 4, Train loss: 1.544, Val loss: 1.481, Epoch time = 157.312s\n",
      "BLEU-4 corpus score = 0.21062424678123656, corpus length = 1000, time elapsed = 7.836866140365601 seconds.\n",
      "Epoch: 5, Train loss: 1.367, Val loss: 1.349, Epoch time = 200.502s\n",
      "BLEU-4 corpus score = 0.2594924151952095, corpus length = 1000, time elapsed = 9.830862522125244 seconds.\n",
      "Epoch: 6, Train loss: 1.263, Val loss: 1.278, Epoch time = 245.455s\n",
      "BLEU-4 corpus score = 0.3048480867112121, corpus length = 1000, time elapsed = 4.865987300872803 seconds.\n",
      "Epoch: 7, Train loss: 1.182, Val loss: 1.243, Epoch time = 285.174s\n",
      "BLEU-4 corpus score = 0.30533030052783183, corpus length = 1000, time elapsed = 4.922598838806152 seconds.\n",
      "Epoch: 8, Train loss: 1.112, Val loss: 1.190, Epoch time = 325.064s\n",
      "BLEU-4 corpus score = 0.33567797777001507, corpus length = 1000, time elapsed = 3.386253595352173 seconds.\n",
      "Epoch: 9, Train loss: 1.052, Val loss: 1.163, Epoch time = 363.295s\n",
      "BLEU-4 corpus score = 0.33893012019082935, corpus length = 1000, time elapsed = 5.671520471572876 seconds.\n",
      "Epoch: 10, Train loss: 1.000, Val loss: 1.148, Epoch time = 403.855s\n",
      "BLEU-4 corpus score = 0.33882445911959513, corpus length = 1000, time elapsed = 3.236591339111328 seconds.\n",
      "Epoch: 11, Train loss: 0.967, Val loss: 1.130, Epoch time = 441.943s\n",
      "BLEU-4 corpus score = 0.3577827120991269, corpus length = 1000, time elapsed = 4.129441976547241 seconds.\n",
      "Epoch: 12, Train loss: 0.936, Val loss: 1.123, Epoch time = 481.074s\n",
      "BLEU-4 corpus score = 0.3403484348345983, corpus length = 1000, time elapsed = 7.0009942054748535 seconds.\n",
      "Epoch: 13, Train loss: 0.911, Val loss: 1.114, Epoch time = 523.417s\n",
      "BLEU-4 corpus score = 0.3518342497307321, corpus length = 1000, time elapsed = 3.4402401447296143 seconds.\n",
      "Epoch: 14, Train loss: 0.896, Val loss: 1.105, Epoch time = 562.043s\n",
      "BLEU-4 corpus score = 0.3536364001852924, corpus length = 1000, time elapsed = 3.4384946823120117 seconds.\n",
      "Epoch: 15, Train loss: 0.873, Val loss: 1.098, Epoch time = 600.707s\n",
      "BLEU-4 corpus score = 0.3519502006720375, corpus length = 1000, time elapsed = 5.113596200942993 seconds.\n",
      "Epoch: 16, Train loss: 0.850, Val loss: 1.098, Epoch time = 641.232s\n",
      "BLEU-4 corpus score = 0.35133221930075903, corpus length = 1000, time elapsed = 6.863463401794434 seconds.\n",
      "Epoch: 17, Train loss: 0.840, Val loss: 1.094, Epoch time = 683.033s\n",
      "BLEU-4 corpus score = 0.358189687627343, corpus length = 1000, time elapsed = 3.471024751663208 seconds.\n",
      "Epoch: 18, Train loss: 0.825, Val loss: 1.097, Epoch time = 721.674s\n",
      "BLEU-4 corpus score = 0.34554376136483345, corpus length = 1000, time elapsed = 6.296818017959595 seconds.\n",
      "Epoch: 19, Train loss: 0.817, Val loss: 1.094, Epoch time = 763.250s\n",
      "BLEU-4 corpus score = 0.35803477132628325, corpus length = 1000, time elapsed = 3.568713665008545 seconds.\n",
      "Epoch: 20, Train loss: 0.805, Val loss: 1.095, Epoch time = 802.025s\n",
      "BLEU-4 corpus score = 0.36105813545504534, corpus length = 1000, time elapsed = 4.190701246261597 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.661, Val loss: 2.773, Epoch time = 35.181s\n",
      "BLEU-4 corpus score = 7.791418537368472e-157, corpus length = 1000, time elapsed = 0.7114739418029785 seconds.\n",
      "Epoch: 2, Train loss: 2.241, Val loss: 1.983, Epoch time = 71.199s\n",
      "BLEU-4 corpus score = 0.09636440726244755, corpus length = 1000, time elapsed = 3.212498903274536 seconds.\n",
      "Epoch: 3, Train loss: 1.765, Val loss: 1.605, Epoch time = 109.772s\n",
      "BLEU-4 corpus score = 0.17913024584681905, corpus length = 1000, time elapsed = 3.3417606353759766 seconds.\n",
      "Epoch: 4, Train loss: 1.492, Val loss: 1.362, Epoch time = 148.178s\n",
      "BLEU-4 corpus score = 0.23515580961099206, corpus length = 1000, time elapsed = 9.187167167663574 seconds.\n",
      "Epoch: 5, Train loss: 1.299, Val loss: 1.235, Epoch time = 192.759s\n",
      "BLEU-4 corpus score = 0.27539746207730925, corpus length = 1000, time elapsed = 8.452530145645142 seconds.\n",
      "Epoch: 6, Train loss: 1.163, Val loss: 1.151, Epoch time = 236.552s\n",
      "BLEU-4 corpus score = 0.32303924678186746, corpus length = 1000, time elapsed = 6.041755199432373 seconds.\n",
      "Epoch: 7, Train loss: 1.079, Val loss: 1.109, Epoch time = 277.486s\n",
      "BLEU-4 corpus score = 0.3461201058356884, corpus length = 1000, time elapsed = 3.639310836791992 seconds.\n",
      "Epoch: 8, Train loss: 1.009, Val loss: 1.062, Epoch time = 317.485s\n",
      "BLEU-4 corpus score = 0.3454292266152283, corpus length = 1000, time elapsed = 4.73222804069519 seconds.\n",
      "Epoch: 9, Train loss: 0.948, Val loss: 1.042, Epoch time = 357.277s\n",
      "BLEU-4 corpus score = 0.35423677525754055, corpus length = 1000, time elapsed = 5.28035044670105 seconds.\n",
      "Epoch: 10, Train loss: 0.902, Val loss: 1.026, Epoch time = 397.564s\n",
      "BLEU-4 corpus score = 0.34589366800712185, corpus length = 1000, time elapsed = 5.627164602279663 seconds.\n",
      "Epoch: 11, Train loss: 0.862, Val loss: 1.013, Epoch time = 438.585s\n",
      "BLEU-4 corpus score = 0.3502503947411178, corpus length = 1000, time elapsed = 6.165649652481079 seconds.\n",
      "Epoch: 12, Train loss: 0.838, Val loss: 1.012, Epoch time = 479.670s\n",
      "BLEU-4 corpus score = 0.3580188968659667, corpus length = 1000, time elapsed = 4.382967948913574 seconds.\n",
      "Epoch: 13, Train loss: 0.807, Val loss: 1.012, Epoch time = 519.255s\n",
      "BLEU-4 corpus score = 0.36177865549330945, corpus length = 1000, time elapsed = 5.007761001586914 seconds.\n",
      "Epoch: 14, Train loss: 0.791, Val loss: 1.006, Epoch time = 559.174s\n",
      "BLEU-4 corpus score = 0.340753476887336, corpus length = 1000, time elapsed = 7.1048455238342285 seconds.\n",
      "Epoch: 15, Train loss: 0.771, Val loss: 1.012, Epoch time = 601.226s\n",
      "BLEU-4 corpus score = 0.38230397901267615, corpus length = 1000, time elapsed = 3.561279296875 seconds.\n",
      "Epoch: 16, Train loss: 0.761, Val loss: 0.998, Epoch time = 640.088s\n",
      "BLEU-4 corpus score = 0.36275586379435826, corpus length = 1000, time elapsed = 5.706096649169922 seconds.\n",
      "Epoch: 17, Train loss: 0.747, Val loss: 0.997, Epoch time = 680.855s\n",
      "BLEU-4 corpus score = 0.3675987477370161, corpus length = 1000, time elapsed = 6.265861749649048 seconds.\n",
      "Epoch: 18, Train loss: 0.736, Val loss: 0.999, Epoch time = 722.172s\n",
      "BLEU-4 corpus score = 0.37329103169324745, corpus length = 1000, time elapsed = 5.469486951828003 seconds.\n",
      "Epoch: 19, Train loss: 0.725, Val loss: 0.995, Epoch time = 763.056s\n",
      "BLEU-4 corpus score = 0.3698296803409365, corpus length = 1000, time elapsed = 5.494863271713257 seconds.\n",
      "Epoch: 20, Train loss: 0.716, Val loss: 0.996, Epoch time = 804.367s\n",
      "BLEU-4 corpus score = 0.36275643521129614, corpus length = 1000, time elapsed = 6.6249165534973145 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.660, Val loss: 2.776, Epoch time = 35.466s\n",
      "BLEU-4 corpus score = 1.439711888933637e-157, corpus length = 1000, time elapsed = 0.7843763828277588 seconds.\n",
      "Epoch: 2, Train loss: 2.246, Val loss: 1.976, Epoch time = 71.759s\n",
      "BLEU-4 corpus score = 0.07119561487239197, corpus length = 1000, time elapsed = 10.35542893409729 seconds.\n",
      "Epoch: 3, Train loss: 1.774, Val loss: 1.608, Epoch time = 117.794s\n",
      "BLEU-4 corpus score = 0.17565419829441792, corpus length = 1000, time elapsed = 3.1494052410125732 seconds.\n",
      "Epoch: 4, Train loss: 1.488, Val loss: 1.370, Epoch time = 156.235s\n",
      "BLEU-4 corpus score = 0.2480178995220415, corpus length = 1000, time elapsed = 7.438644886016846 seconds.\n",
      "Epoch: 5, Train loss: 1.299, Val loss: 1.234, Epoch time = 199.101s\n",
      "BLEU-4 corpus score = 0.3027854163324336, corpus length = 1000, time elapsed = 3.1514322757720947 seconds.\n",
      "Epoch: 6, Train loss: 1.178, Val loss: 1.183, Epoch time = 237.377s\n",
      "BLEU-4 corpus score = 0.3174782672616167, corpus length = 1000, time elapsed = 6.213623046875 seconds.\n",
      "Epoch: 7, Train loss: 1.086, Val loss: 1.117, Epoch time = 278.519s\n",
      "BLEU-4 corpus score = 0.3410093699617297, corpus length = 1000, time elapsed = 3.0809481143951416 seconds.\n",
      "Epoch: 8, Train loss: 1.012, Val loss: 1.074, Epoch time = 316.750s\n",
      "BLEU-4 corpus score = 0.34919760586239207, corpus length = 1000, time elapsed = 6.053677320480347 seconds.\n",
      "Epoch: 9, Train loss: 0.954, Val loss: 1.042, Epoch time = 358.023s\n",
      "BLEU-4 corpus score = 0.35418265128679277, corpus length = 1000, time elapsed = 5.85610032081604 seconds.\n",
      "Epoch: 10, Train loss: 0.900, Val loss: 1.024, Epoch time = 399.145s\n",
      "BLEU-4 corpus score = 0.3740947389665824, corpus length = 1000, time elapsed = 3.9764952659606934 seconds.\n",
      "Epoch: 11, Train loss: 0.866, Val loss: 1.014, Epoch time = 438.209s\n",
      "BLEU-4 corpus score = 0.35703154356692773, corpus length = 1000, time elapsed = 6.001033067703247 seconds.\n",
      "Epoch: 12, Train loss: 0.835, Val loss: 1.001, Epoch time = 479.089s\n",
      "BLEU-4 corpus score = 0.3604549158381636, corpus length = 1000, time elapsed = 5.323026895523071 seconds.\n",
      "Epoch: 13, Train loss: 0.811, Val loss: 0.998, Epoch time = 519.360s\n",
      "BLEU-4 corpus score = 0.3692557968238569, corpus length = 1000, time elapsed = 5.706422328948975 seconds.\n",
      "Epoch: 14, Train loss: 0.792, Val loss: 1.000, Epoch time = 560.092s\n",
      "BLEU-4 corpus score = 0.3715242251805882, corpus length = 1000, time elapsed = 6.119062185287476 seconds.\n",
      "Epoch: 15, Train loss: 0.774, Val loss: 0.996, Epoch time = 601.280s\n",
      "BLEU-4 corpus score = 0.37373432152225033, corpus length = 1000, time elapsed = 4.173505067825317 seconds.\n",
      "Epoch: 16, Train loss: 0.755, Val loss: 0.995, Epoch time = 640.910s\n",
      "BLEU-4 corpus score = 0.3701312405659723, corpus length = 1000, time elapsed = 6.109140872955322 seconds.\n",
      "Epoch: 17, Train loss: 0.744, Val loss: 0.992, Epoch time = 682.161s\n",
      "BLEU-4 corpus score = 0.36675750825343334, corpus length = 1000, time elapsed = 6.212981462478638 seconds.\n",
      "Epoch: 18, Train loss: 0.735, Val loss: 0.992, Epoch time = 723.434s\n",
      "BLEU-4 corpus score = 0.3716455124389691, corpus length = 1000, time elapsed = 4.449838876724243 seconds.\n",
      "Epoch: 19, Train loss: 0.723, Val loss: 0.992, Epoch time = 763.175s\n",
      "BLEU-4 corpus score = 0.379463509397961, corpus length = 1000, time elapsed = 5.931495904922485 seconds.\n",
      "Epoch: 20, Train loss: 0.715, Val loss: 0.997, Epoch time = 804.697s\n",
      "BLEU-4 corpus score = 0.3720691094927288, corpus length = 1000, time elapsed = 6.886420488357544 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.696, Val loss: 2.763, Epoch time = 38.090s\n",
      "BLEU-4 corpus score = 5.1826562212955866e-157, corpus length = 1000, time elapsed = 0.874351978302002 seconds.\n",
      "Epoch: 2, Train loss: 2.244, Val loss: 2.015, Epoch time = 77.122s\n",
      "BLEU-4 corpus score = 0.05982782595185944, corpus length = 1000, time elapsed = 18.59716820716858 seconds.\n",
      "Epoch: 3, Train loss: 1.819, Val loss: 1.754, Epoch time = 134.072s\n",
      "BLEU-4 corpus score = 0.10582356645972382, corpus length = 1000, time elapsed = 13.809054374694824 seconds.\n",
      "Epoch: 4, Train loss: 1.610, Val loss: 1.587, Epoch time = 185.566s\n",
      "BLEU-4 corpus score = 0.17821947597920793, corpus length = 1000, time elapsed = 8.105504274368286 seconds.\n",
      "Epoch: 5, Train loss: 1.477, Val loss: 1.484, Epoch time = 231.479s\n",
      "BLEU-4 corpus score = 0.18156338000761474, corpus length = 1000, time elapsed = 12.939554214477539 seconds.\n",
      "Epoch: 6, Train loss: 1.359, Val loss: 1.385, Epoch time = 282.389s\n",
      "BLEU-4 corpus score = 0.24397063363982102, corpus length = 1000, time elapsed = 6.249026536941528 seconds.\n",
      "Epoch: 7, Train loss: 1.262, Val loss: 1.325, Epoch time = 326.562s\n",
      "BLEU-4 corpus score = 0.24704911523477993, corpus length = 1000, time elapsed = 9.568378448486328 seconds.\n",
      "Epoch: 8, Train loss: 1.194, Val loss: 1.270, Epoch time = 373.962s\n",
      "BLEU-4 corpus score = 0.2800291946422586, corpus length = 1000, time elapsed = 6.38109278678894 seconds.\n",
      "Epoch: 9, Train loss: 1.123, Val loss: 1.229, Epoch time = 418.310s\n",
      "BLEU-4 corpus score = 0.30076692485873274, corpus length = 1000, time elapsed = 4.869772434234619 seconds.\n",
      "Epoch: 10, Train loss: 1.073, Val loss: 1.202, Epoch time = 460.779s\n",
      "BLEU-4 corpus score = 0.2956187494971693, corpus length = 1000, time elapsed = 8.139019966125488 seconds.\n",
      "Epoch: 11, Train loss: 1.020, Val loss: 1.180, Epoch time = 506.744s\n",
      "BLEU-4 corpus score = 0.3120966003929719, corpus length = 1000, time elapsed = 5.0184714794158936 seconds.\n",
      "Epoch: 12, Train loss: 0.985, Val loss: 1.161, Epoch time = 549.227s\n",
      "BLEU-4 corpus score = 0.3075043624267155, corpus length = 1000, time elapsed = 7.09760594367981 seconds.\n",
      "Epoch: 13, Train loss: 0.957, Val loss: 1.147, Epoch time = 594.673s\n",
      "BLEU-4 corpus score = 0.32255586422474813, corpus length = 1000, time elapsed = 7.3833417892456055 seconds.\n",
      "Epoch: 14, Train loss: 0.932, Val loss: 1.143, Epoch time = 639.720s\n",
      "BLEU-4 corpus score = 0.3236695108922988, corpus length = 1000, time elapsed = 5.126162767410278 seconds.\n",
      "Epoch: 15, Train loss: 0.909, Val loss: 1.131, Epoch time = 683.041s\n",
      "BLEU-4 corpus score = 0.3313334299649608, corpus length = 1000, time elapsed = 4.870906352996826 seconds.\n",
      "Epoch: 16, Train loss: 0.889, Val loss: 1.128, Epoch time = 725.886s\n",
      "BLEU-4 corpus score = 0.3346393606404153, corpus length = 1000, time elapsed = 5.701664924621582 seconds.\n",
      "Epoch: 17, Train loss: 0.872, Val loss: 1.118, Epoch time = 769.709s\n",
      "BLEU-4 corpus score = 0.3166109485230395, corpus length = 1000, time elapsed = 12.463416576385498 seconds.\n",
      "Epoch: 18, Train loss: 0.852, Val loss: 1.115, Epoch time = 819.874s\n",
      "BLEU-4 corpus score = 0.3356977153186392, corpus length = 1000, time elapsed = 5.368714809417725 seconds.\n",
      "Epoch: 19, Train loss: 0.840, Val loss: 1.111, Epoch time = 863.344s\n",
      "BLEU-4 corpus score = 0.33441127346525834, corpus length = 1000, time elapsed = 9.879637718200684 seconds.\n",
      "Epoch: 20, Train loss: 0.830, Val loss: 1.110, Epoch time = 911.362s\n",
      "BLEU-4 corpus score = 0.3358516274901011, corpus length = 1000, time elapsed = 10.907539367675781 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.663, Val loss: 2.781, Epoch time = 38.042s\n",
      "BLEU-4 corpus score = 1.2522532528475791e-155, corpus length = 1000, time elapsed = 1.487955093383789 seconds.\n",
      "Epoch: 2, Train loss: 2.252, Val loss: 2.022, Epoch time = 77.821s\n",
      "BLEU-4 corpus score = 0.048282555567871484, corpus length = 1000, time elapsed = 19.191094636917114 seconds.\n",
      "Epoch: 3, Train loss: 1.814, Val loss: 1.689, Epoch time = 135.382s\n",
      "BLEU-4 corpus score = 0.13502133114089182, corpus length = 1000, time elapsed = 11.447661638259888 seconds.\n",
      "Epoch: 4, Train loss: 1.552, Val loss: 1.483, Epoch time = 184.895s\n",
      "BLEU-4 corpus score = 0.22779970132970634, corpus length = 1000, time elapsed = 4.778664588928223 seconds.\n",
      "Epoch: 5, Train loss: 1.355, Val loss: 1.330, Epoch time = 227.855s\n",
      "BLEU-4 corpus score = 0.27738065836803094, corpus length = 1000, time elapsed = 4.009430408477783 seconds.\n",
      "Epoch: 6, Train loss: 1.219, Val loss: 1.277, Epoch time = 270.125s\n",
      "BLEU-4 corpus score = 0.30968338839740767, corpus length = 1000, time elapsed = 4.4104695320129395 seconds.\n",
      "Epoch: 7, Train loss: 1.143, Val loss: 1.206, Epoch time = 312.522s\n",
      "BLEU-4 corpus score = 0.3282323893942834, corpus length = 1000, time elapsed = 3.5425219535827637 seconds.\n",
      "Epoch: 8, Train loss: 1.066, Val loss: 1.159, Epoch time = 354.151s\n",
      "BLEU-4 corpus score = 0.3384134754438019, corpus length = 1000, time elapsed = 4.339405536651611 seconds.\n",
      "Epoch: 9, Train loss: 1.002, Val loss: 1.135, Epoch time = 396.304s\n",
      "BLEU-4 corpus score = 0.3491378752641076, corpus length = 1000, time elapsed = 4.026394605636597 seconds.\n",
      "Epoch: 10, Train loss: 0.956, Val loss: 1.114, Epoch time = 438.024s\n",
      "BLEU-4 corpus score = 0.34990234519774394, corpus length = 1000, time elapsed = 4.310230493545532 seconds.\n",
      "Epoch: 11, Train loss: 0.922, Val loss: 1.103, Epoch time = 480.279s\n",
      "BLEU-4 corpus score = 0.3489848534729726, corpus length = 1000, time elapsed = 7.309816360473633 seconds.\n",
      "Epoch: 12, Train loss: 0.884, Val loss: 1.098, Epoch time = 527.194s\n",
      "BLEU-4 corpus score = 0.3518243974206852, corpus length = 1000, time elapsed = 5.904358625411987 seconds.\n",
      "Epoch: 13, Train loss: 0.862, Val loss: 1.098, Epoch time = 571.097s\n",
      "BLEU-4 corpus score = 0.360455360767622, corpus length = 1000, time elapsed = 4.432875633239746 seconds.\n",
      "Epoch: 14, Train loss: 0.839, Val loss: 1.081, Epoch time = 613.802s\n",
      "BLEU-4 corpus score = 0.34538071183741004, corpus length = 1000, time elapsed = 6.745447874069214 seconds.\n",
      "Epoch: 15, Train loss: 0.823, Val loss: 1.082, Epoch time = 658.445s\n",
      "BLEU-4 corpus score = 0.3489976479170536, corpus length = 1000, time elapsed = 5.227824687957764 seconds.\n",
      "Epoch: 16, Train loss: 0.807, Val loss: 1.080, Epoch time = 701.901s\n",
      "BLEU-4 corpus score = 0.36245351991851965, corpus length = 1000, time elapsed = 6.4406256675720215 seconds.\n",
      "Epoch: 17, Train loss: 0.790, Val loss: 1.082, Epoch time = 745.952s\n",
      "BLEU-4 corpus score = 0.35621625845520927, corpus length = 1000, time elapsed = 5.959639072418213 seconds.\n",
      "Epoch: 18, Train loss: 0.774, Val loss: 1.077, Epoch time = 789.955s\n",
      "BLEU-4 corpus score = 0.3647614155393057, corpus length = 1000, time elapsed = 3.806122303009033 seconds.\n",
      "Epoch: 19, Train loss: 0.765, Val loss: 1.078, Epoch time = 832.095s\n",
      "BLEU-4 corpus score = 0.3561298931572862, corpus length = 1000, time elapsed = 7.084052324295044 seconds.\n",
      "Epoch: 20, Train loss: 0.753, Val loss: 1.074, Epoch time = 877.351s\n",
      "BLEU-4 corpus score = 0.35526574072693373, corpus length = 1000, time elapsed = 5.217096567153931 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.673, Val loss: 2.769, Epoch time = 36.209s\n",
      "BLEU-4 corpus score = 6.2948097205765e-157, corpus length = 1000, time elapsed = 0.744037389755249 seconds.\n",
      "Epoch: 2, Train loss: 2.244, Val loss: 1.974, Epoch time = 72.764s\n",
      "BLEU-4 corpus score = 0.0798329642733273, corpus length = 1000, time elapsed = 11.146156311035156 seconds.\n",
      "Epoch: 3, Train loss: 1.776, Val loss: 1.666, Epoch time = 120.080s\n",
      "BLEU-4 corpus score = 0.160995200302702, corpus length = 1000, time elapsed = 4.424982070922852 seconds.\n",
      "Epoch: 4, Train loss: 1.524, Val loss: 1.457, Epoch time = 160.531s\n",
      "BLEU-4 corpus score = 0.23127543474771595, corpus length = 1000, time elapsed = 5.348527431488037 seconds.\n",
      "Epoch: 5, Train loss: 1.356, Val loss: 1.337, Epoch time = 201.733s\n",
      "BLEU-4 corpus score = 0.28654748286167686, corpus length = 1000, time elapsed = 3.282160758972168 seconds.\n",
      "Epoch: 6, Train loss: 1.229, Val loss: 1.272, Epoch time = 240.987s\n",
      "BLEU-4 corpus score = 0.2855003700155144, corpus length = 1000, time elapsed = 4.762127637863159 seconds.\n",
      "Epoch: 7, Train loss: 1.150, Val loss: 1.210, Epoch time = 281.687s\n",
      "BLEU-4 corpus score = 0.3159073633789511, corpus length = 1000, time elapsed = 5.127480506896973 seconds.\n",
      "Epoch: 8, Train loss: 1.075, Val loss: 1.166, Epoch time = 323.017s\n",
      "BLEU-4 corpus score = 0.3332734117254331, corpus length = 1000, time elapsed = 4.719760894775391 seconds.\n",
      "Epoch: 9, Train loss: 1.018, Val loss: 1.137, Epoch time = 363.879s\n",
      "BLEU-4 corpus score = 0.34761506343742665, corpus length = 1000, time elapsed = 4.596055030822754 seconds.\n",
      "Epoch: 10, Train loss: 0.965, Val loss: 1.122, Epoch time = 404.419s\n",
      "BLEU-4 corpus score = 0.3488912610646047, corpus length = 1000, time elapsed = 4.632415056228638 seconds.\n",
      "Epoch: 11, Train loss: 0.931, Val loss: 1.100, Epoch time = 445.052s\n",
      "BLEU-4 corpus score = 0.34223507312415075, corpus length = 1000, time elapsed = 3.992213726043701 seconds.\n",
      "Epoch: 12, Train loss: 0.911, Val loss: 1.102, Epoch time = 485.120s\n",
      "BLEU-4 corpus score = 0.35812609330906225, corpus length = 1000, time elapsed = 4.802388668060303 seconds.\n",
      "Epoch: 13, Train loss: 0.876, Val loss: 1.090, Epoch time = 525.677s\n",
      "BLEU-4 corpus score = 0.3574439117575842, corpus length = 1000, time elapsed = 3.8773162364959717 seconds.\n",
      "Epoch: 14, Train loss: 0.850, Val loss: 1.084, Epoch time = 565.318s\n",
      "BLEU-4 corpus score = 0.367886495879682, corpus length = 1000, time elapsed = 4.510620832443237 seconds.\n",
      "Epoch: 15, Train loss: 0.833, Val loss: 1.085, Epoch time = 605.734s\n",
      "BLEU-4 corpus score = 0.37166582444137386, corpus length = 1000, time elapsed = 4.73145055770874 seconds.\n",
      "Epoch: 16, Train loss: 0.822, Val loss: 1.071, Epoch time = 646.505s\n",
      "BLEU-4 corpus score = 0.3609916418540195, corpus length = 1000, time elapsed = 4.11403489112854 seconds.\n",
      "Epoch: 17, Train loss: 0.804, Val loss: 1.067, Epoch time = 686.642s\n",
      "BLEU-4 corpus score = 0.3440680607762036, corpus length = 1000, time elapsed = 7.076127052307129 seconds.\n",
      "Epoch: 18, Train loss: 0.784, Val loss: 1.073, Epoch time = 729.416s\n",
      "BLEU-4 corpus score = 0.3722473182953154, corpus length = 1000, time elapsed = 3.7395925521850586 seconds.\n",
      "Epoch: 19, Train loss: 0.783, Val loss: 1.061, Epoch time = 769.301s\n",
      "BLEU-4 corpus score = 0.3650609086411739, corpus length = 1000, time elapsed = 4.610804796218872 seconds.\n",
      "Epoch: 20, Train loss: 0.779, Val loss: 1.066, Epoch time = 809.753s\n",
      "BLEU-4 corpus score = 0.3586398643211887, corpus length = 1000, time elapsed = 4.829837083816528 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.673, Val loss: 2.773, Epoch time = 36.036s\n",
      "BLEU-4 corpus score = 4.2849027717056804e-234, corpus length = 1000, time elapsed = 0.6271286010742188 seconds.\n",
      "Epoch: 2, Train loss: 2.225, Val loss: 1.970, Epoch time = 72.904s\n",
      "BLEU-4 corpus score = 0.07247755686505145, corpus length = 1000, time elapsed = 18.658090829849243 seconds.\n",
      "Epoch: 3, Train loss: 1.785, Val loss: 1.662, Epoch time = 127.712s\n",
      "BLEU-4 corpus score = 0.13741755828654367, corpus length = 1000, time elapsed = 12.788150072097778 seconds.\n",
      "Epoch: 4, Train loss: 1.537, Val loss: 1.461, Epoch time = 176.408s\n",
      "BLEU-4 corpus score = 0.21433605831688463, corpus length = 1000, time elapsed = 5.651575803756714 seconds.\n",
      "Epoch: 5, Train loss: 1.367, Val loss: 1.338, Epoch time = 218.164s\n",
      "BLEU-4 corpus score = 0.27001358243780976, corpus length = 1000, time elapsed = 5.934446811676025 seconds.\n",
      "Epoch: 6, Train loss: 1.246, Val loss: 1.289, Epoch time = 260.312s\n",
      "BLEU-4 corpus score = 0.2954228417791903, corpus length = 1000, time elapsed = 8.414674997329712 seconds.\n",
      "Epoch: 7, Train loss: 1.155, Val loss: 1.226, Epoch time = 304.501s\n",
      "BLEU-4 corpus score = 0.28970476308622334, corpus length = 1000, time elapsed = 8.763403177261353 seconds.\n",
      "Epoch: 8, Train loss: 1.087, Val loss: 1.177, Epoch time = 349.144s\n",
      "BLEU-4 corpus score = 0.3343845703024541, corpus length = 1000, time elapsed = 5.879108667373657 seconds.\n",
      "Epoch: 9, Train loss: 1.023, Val loss: 1.147, Epoch time = 390.843s\n",
      "BLEU-4 corpus score = 0.3119350181497074, corpus length = 1000, time elapsed = 9.03474473953247 seconds.\n",
      "Epoch: 10, Train loss: 0.985, Val loss: 1.126, Epoch time = 435.663s\n",
      "BLEU-4 corpus score = 0.3471602280643856, corpus length = 1000, time elapsed = 3.9380195140838623 seconds.\n",
      "Epoch: 11, Train loss: 0.942, Val loss: 1.115, Epoch time = 475.359s\n",
      "BLEU-4 corpus score = 0.3379135939736262, corpus length = 1000, time elapsed = 6.294549226760864 seconds.\n",
      "Epoch: 12, Train loss: 0.911, Val loss: 1.104, Epoch time = 517.591s\n",
      "BLEU-4 corpus score = 0.35493743544108197, corpus length = 1000, time elapsed = 4.377143383026123 seconds.\n",
      "Epoch: 13, Train loss: 0.886, Val loss: 1.087, Epoch time = 558.010s\n",
      "BLEU-4 corpus score = 0.349634611052795, corpus length = 1000, time elapsed = 6.404696702957153 seconds.\n",
      "Epoch: 14, Train loss: 0.867, Val loss: 1.077, Epoch time = 600.573s\n",
      "BLEU-4 corpus score = 0.3583423078781165, corpus length = 1000, time elapsed = 6.026360034942627 seconds.\n",
      "Epoch: 15, Train loss: 0.848, Val loss: 1.070, Epoch time = 642.756s\n",
      "BLEU-4 corpus score = 0.3734501365567218, corpus length = 1000, time elapsed = 3.804824113845825 seconds.\n",
      "Epoch: 16, Train loss: 0.825, Val loss: 1.081, Epoch time = 682.294s\n",
      "BLEU-4 corpus score = 0.366665203253823, corpus length = 1000, time elapsed = 5.2318115234375 seconds.\n",
      "Epoch: 17, Train loss: 0.813, Val loss: 1.065, Epoch time = 723.722s\n",
      "BLEU-4 corpus score = 0.35654731810100854, corpus length = 1000, time elapsed = 5.422560691833496 seconds.\n",
      "Epoch: 18, Train loss: 0.806, Val loss: 1.065, Epoch time = 765.140s\n",
      "BLEU-4 corpus score = 0.36713093028439847, corpus length = 1000, time elapsed = 4.475645542144775 seconds.\n",
      "Epoch: 19, Train loss: 0.792, Val loss: 1.067, Epoch time = 805.587s\n",
      "BLEU-4 corpus score = 0.37648094194010767, corpus length = 1000, time elapsed = 3.8162033557891846 seconds.\n",
      "Epoch: 20, Train loss: 0.788, Val loss: 1.065, Epoch time = 845.615s\n",
      "BLEU-4 corpus score = 0.35740362238854906, corpus length = 1000, time elapsed = 6.324285507202148 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.676, Val loss: 2.764, Epoch time = 36.157s\n",
      "BLEU-4 corpus score = 4.2849027717056804e-234, corpus length = 1000, time elapsed = 0.6397843360900879 seconds.\n",
      "Epoch: 2, Train loss: 2.245, Val loss: 1.992, Epoch time = 73.207s\n",
      "BLEU-4 corpus score = 0.043457547786981014, corpus length = 1000, time elapsed = 20.619227647781372 seconds.\n",
      "Epoch: 3, Train loss: 1.786, Val loss: 1.630, Epoch time = 130.057s\n",
      "BLEU-4 corpus score = 0.15431430865491794, corpus length = 1000, time elapsed = 9.79841947555542 seconds.\n",
      "Epoch: 4, Train loss: 1.509, Val loss: 1.391, Epoch time = 176.373s\n",
      "BLEU-4 corpus score = 0.23311106818659727, corpus length = 1000, time elapsed = 7.199995279312134 seconds.\n",
      "Epoch: 5, Train loss: 1.308, Val loss: 1.261, Epoch time = 219.621s\n",
      "BLEU-4 corpus score = 0.2929914314710987, corpus length = 1000, time elapsed = 3.96584153175354 seconds.\n",
      "Epoch: 6, Train loss: 1.176, Val loss: 1.176, Epoch time = 260.153s\n",
      "BLEU-4 corpus score = 0.3007307991714427, corpus length = 1000, time elapsed = 8.304579496383667 seconds.\n",
      "Epoch: 7, Train loss: 1.083, Val loss: 1.114, Epoch time = 304.575s\n",
      "BLEU-4 corpus score = 0.31019287352331976, corpus length = 1000, time elapsed = 7.567921876907349 seconds.\n",
      "Epoch: 8, Train loss: 1.010, Val loss: 1.083, Epoch time = 348.213s\n",
      "BLEU-4 corpus score = 0.31212364573541723, corpus length = 1000, time elapsed = 7.260112762451172 seconds.\n",
      "Epoch: 9, Train loss: 0.948, Val loss: 1.041, Epoch time = 392.326s\n",
      "BLEU-4 corpus score = 0.3460543180668459, corpus length = 1000, time elapsed = 5.42118501663208 seconds.\n",
      "Epoch: 10, Train loss: 0.894, Val loss: 1.019, Epoch time = 433.632s\n",
      "BLEU-4 corpus score = 0.3509700598728787, corpus length = 1000, time elapsed = 6.2195820808410645 seconds.\n",
      "Epoch: 11, Train loss: 0.848, Val loss: 1.009, Epoch time = 476.536s\n",
      "BLEU-4 corpus score = 0.35243368094036504, corpus length = 1000, time elapsed = 7.093350410461426 seconds.\n",
      "Epoch: 12, Train loss: 0.820, Val loss: 0.997, Epoch time = 519.851s\n",
      "BLEU-4 corpus score = 0.35569403274512934, corpus length = 1000, time elapsed = 6.531275033950806 seconds.\n",
      "Epoch: 13, Train loss: 0.792, Val loss: 0.989, Epoch time = 562.941s\n",
      "BLEU-4 corpus score = 0.3556506874888428, corpus length = 1000, time elapsed = 6.085501194000244 seconds.\n",
      "Epoch: 14, Train loss: 0.773, Val loss: 0.993, Epoch time = 605.550s\n",
      "BLEU-4 corpus score = 0.366327629409251, corpus length = 1000, time elapsed = 4.507071256637573 seconds.\n",
      "Epoch: 15, Train loss: 0.748, Val loss: 0.980, Epoch time = 646.271s\n",
      "BLEU-4 corpus score = 0.37471426569058247, corpus length = 1000, time elapsed = 6.24756932258606 seconds.\n",
      "Epoch: 16, Train loss: 0.739, Val loss: 0.981, Epoch time = 688.381s\n",
      "BLEU-4 corpus score = 0.3772151853012912, corpus length = 1000, time elapsed = 5.5264246463775635 seconds.\n",
      "Epoch: 17, Train loss: 0.722, Val loss: 0.983, Epoch time = 730.160s\n",
      "BLEU-4 corpus score = 0.3812991873221937, corpus length = 1000, time elapsed = 3.719134569168091 seconds.\n",
      "Epoch: 18, Train loss: 0.711, Val loss: 0.982, Epoch time = 769.683s\n",
      "BLEU-4 corpus score = 0.37495335613440745, corpus length = 1000, time elapsed = 5.169209718704224 seconds.\n",
      "Epoch: 19, Train loss: 0.699, Val loss: 0.981, Epoch time = 810.726s\n",
      "BLEU-4 corpus score = 0.37627676137739935, corpus length = 1000, time elapsed = 5.36758017539978 seconds.\n",
      "Epoch: 20, Train loss: 0.690, Val loss: 0.983, Epoch time = 852.061s\n",
      "BLEU-4 corpus score = 0.3820492632187981, corpus length = 1000, time elapsed = 4.668666839599609 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.671, Val loss: 2.759, Epoch time = 36.015s\n",
      "BLEU-4 corpus score = 2.4545298952770884e-155, corpus length = 1000, time elapsed = 1.0541892051696777 seconds.\n",
      "Epoch: 2, Train loss: 2.251, Val loss: 2.018, Epoch time = 73.129s\n",
      "BLEU-4 corpus score = 0.07001190483330087, corpus length = 1000, time elapsed = 4.324594020843506 seconds.\n",
      "Epoch: 3, Train loss: 1.782, Val loss: 1.623, Epoch time = 113.265s\n",
      "BLEU-4 corpus score = 0.15393683664480154, corpus length = 1000, time elapsed = 8.980250358581543 seconds.\n",
      "Epoch: 4, Train loss: 1.496, Val loss: 1.388, Epoch time = 158.129s\n",
      "BLEU-4 corpus score = 0.20139405074387798, corpus length = 1000, time elapsed = 14.512818336486816 seconds.\n",
      "Epoch: 5, Train loss: 1.295, Val loss: 1.242, Epoch time = 209.154s\n",
      "BLEU-4 corpus score = 0.29793481444518305, corpus length = 1000, time elapsed = 4.581458568572998 seconds.\n",
      "Epoch: 6, Train loss: 1.164, Val loss: 1.151, Epoch time = 249.912s\n",
      "BLEU-4 corpus score = 0.30917411552194346, corpus length = 1000, time elapsed = 7.126769065856934 seconds.\n",
      "Epoch: 7, Train loss: 1.071, Val loss: 1.111, Epoch time = 293.238s\n",
      "BLEU-4 corpus score = 0.339865274492093, corpus length = 1000, time elapsed = 5.19972038269043 seconds.\n",
      "Epoch: 8, Train loss: 1.003, Val loss: 1.066, Epoch time = 334.824s\n",
      "BLEU-4 corpus score = 0.34876351305445924, corpus length = 1000, time elapsed = 4.615036725997925 seconds.\n",
      "Epoch: 9, Train loss: 0.931, Val loss: 1.043, Epoch time = 376.418s\n",
      "BLEU-4 corpus score = 0.3646694796111269, corpus length = 1000, time elapsed = 3.567732572555542 seconds.\n",
      "Epoch: 10, Train loss: 0.889, Val loss: 1.015, Epoch time = 415.961s\n",
      "BLEU-4 corpus score = 0.3395456175565479, corpus length = 1000, time elapsed = 7.941528558731079 seconds.\n",
      "Epoch: 11, Train loss: 0.852, Val loss: 1.012, Epoch time = 460.172s\n",
      "BLEU-4 corpus score = 0.36212444516173437, corpus length = 1000, time elapsed = 6.154749393463135 seconds.\n",
      "Epoch: 12, Train loss: 0.815, Val loss: 1.007, Epoch time = 502.442s\n",
      "BLEU-4 corpus score = 0.3851506740790573, corpus length = 1000, time elapsed = 3.6959211826324463 seconds.\n",
      "Epoch: 13, Train loss: 0.794, Val loss: 0.992, Epoch time = 542.319s\n",
      "BLEU-4 corpus score = 0.37119828260731047, corpus length = 1000, time elapsed = 6.591938495635986 seconds.\n",
      "Epoch: 14, Train loss: 0.773, Val loss: 0.990, Epoch time = 584.923s\n",
      "BLEU-4 corpus score = 0.3637925906495175, corpus length = 1000, time elapsed = 7.535468339920044 seconds.\n",
      "Epoch: 15, Train loss: 0.748, Val loss: 0.990, Epoch time = 628.681s\n",
      "BLEU-4 corpus score = 0.3658724739946646, corpus length = 1000, time elapsed = 5.332519292831421 seconds.\n",
      "Epoch: 16, Train loss: 0.733, Val loss: 0.986, Epoch time = 670.151s\n",
      "BLEU-4 corpus score = 0.38302842544107424, corpus length = 1000, time elapsed = 4.92261528968811 seconds.\n",
      "Epoch: 17, Train loss: 0.723, Val loss: 0.985, Epoch time = 711.250s\n",
      "BLEU-4 corpus score = 0.3725983673428991, corpus length = 1000, time elapsed = 4.885470390319824 seconds.\n",
      "Epoch: 18, Train loss: 0.708, Val loss: 0.989, Epoch time = 752.090s\n",
      "BLEU-4 corpus score = 0.3735376230865687, corpus length = 1000, time elapsed = 4.898958683013916 seconds.\n",
      "Epoch: 19, Train loss: 0.697, Val loss: 0.986, Epoch time = 793.572s\n",
      "BLEU-4 corpus score = 0.3756181419941443, corpus length = 1000, time elapsed = 5.48197865486145 seconds.\n",
      "Epoch: 20, Train loss: 0.690, Val loss: 0.978, Epoch time = 835.161s\n",
      "BLEU-4 corpus score = 0.37496205145920614, corpus length = 1000, time elapsed = 5.201093435287476 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.493, Val loss: 2.570, Epoch time = 39.058s\n",
      "BLEU-4 corpus score = 0.010402400941947082, corpus length = 1000, time elapsed = 1.9481480121612549 seconds.\n",
      "Epoch: 2, Train loss: 2.111, Val loss: 1.913, Epoch time = 80.243s\n",
      "BLEU-4 corpus score = 0.10358516984144221, corpus length = 1000, time elapsed = 6.017042636871338 seconds.\n",
      "Epoch: 3, Train loss: 1.719, Val loss: 1.642, Epoch time = 125.456s\n",
      "BLEU-4 corpus score = 0.16706844382638675, corpus length = 1000, time elapsed = 8.147824764251709 seconds.\n",
      "Epoch: 4, Train loss: 1.497, Val loss: 1.444, Epoch time = 172.558s\n",
      "BLEU-4 corpus score = 0.21632195343603786, corpus length = 1000, time elapsed = 10.11676812171936 seconds.\n",
      "Epoch: 5, Train loss: 1.324, Val loss: 1.336, Epoch time = 221.526s\n",
      "BLEU-4 corpus score = 0.26750609918590923, corpus length = 1000, time elapsed = 5.086410045623779 seconds.\n",
      "Epoch: 6, Train loss: 1.209, Val loss: 1.267, Epoch time = 266.614s\n",
      "BLEU-4 corpus score = 0.2819916391938505, corpus length = 1000, time elapsed = 7.007906436920166 seconds.\n",
      "Epoch: 7, Train loss: 1.124, Val loss: 1.233, Epoch time = 312.908s\n",
      "BLEU-4 corpus score = 0.2897007416594899, corpus length = 1000, time elapsed = 3.945321559906006 seconds.\n",
      "Epoch: 8, Train loss: 1.047, Val loss: 1.191, Epoch time = 356.146s\n",
      "BLEU-4 corpus score = 0.2888387237647474, corpus length = 1000, time elapsed = 5.182823657989502 seconds.\n",
      "Epoch: 9, Train loss: 0.983, Val loss: 1.162, Epoch time = 400.793s\n",
      "BLEU-4 corpus score = 0.3090101055404028, corpus length = 1000, time elapsed = 5.368577718734741 seconds.\n",
      "Epoch: 10, Train loss: 0.928, Val loss: 1.152, Epoch time = 445.436s\n",
      "BLEU-4 corpus score = 0.30025158325507856, corpus length = 1000, time elapsed = 7.374896764755249 seconds.\n",
      "Epoch: 11, Train loss: 0.873, Val loss: 1.142, Epoch time = 492.279s\n",
      "BLEU-4 corpus score = 0.31704597696342973, corpus length = 1000, time elapsed = 5.513762712478638 seconds.\n",
      "Epoch: 12, Train loss: 0.834, Val loss: 1.141, Epoch time = 537.033s\n",
      "BLEU-4 corpus score = 0.3251967938291206, corpus length = 1000, time elapsed = 4.066501617431641 seconds.\n",
      "Epoch: 13, Train loss: 0.808, Val loss: 1.137, Epoch time = 581.292s\n",
      "BLEU-4 corpus score = 0.30081540970780724, corpus length = 1000, time elapsed = 7.414921522140503 seconds.\n",
      "Epoch: 14, Train loss: 0.783, Val loss: 1.142, Epoch time = 629.040s\n",
      "BLEU-4 corpus score = 0.3173196923573454, corpus length = 1000, time elapsed = 4.087352275848389 seconds.\n",
      "Epoch: 15, Train loss: 0.762, Val loss: 1.139, Epoch time = 672.115s\n",
      "BLEU-4 corpus score = 0.3211798036822112, corpus length = 1000, time elapsed = 4.143561363220215 seconds.\n",
      "Epoch: 16, Train loss: 0.733, Val loss: 1.142, Epoch time = 715.241s\n",
      "BLEU-4 corpus score = 0.3066914662174938, corpus length = 1000, time elapsed = 9.720816612243652 seconds.\n",
      "Epoch: 17, Train loss: 0.718, Val loss: 1.148, Epoch time = 764.041s\n",
      "BLEU-4 corpus score = 0.3332719847148252, corpus length = 1000, time elapsed = 3.9146230220794678 seconds.\n",
      "Epoch: 18, Train loss: 0.697, Val loss: 1.148, Epoch time = 806.950s\n",
      "BLEU-4 corpus score = 0.3259307133899765, corpus length = 1000, time elapsed = 5.18573260307312 seconds.\n",
      "Epoch: 19, Train loss: 0.687, Val loss: 1.149, Epoch time = 851.047s\n",
      "BLEU-4 corpus score = 0.3198124753758377, corpus length = 1000, time elapsed = 5.589737415313721 seconds.\n",
      "Epoch: 20, Train loss: 0.671, Val loss: 1.152, Epoch time = 895.616s\n",
      "BLEU-4 corpus score = 0.3278988304502665, corpus length = 1000, time elapsed = 4.463107585906982 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.506, Val loss: 2.536, Epoch time = 39.623s\n",
      "BLEU-4 corpus score = 0.04441236559439199, corpus length = 1000, time elapsed = 1.373098373413086 seconds.\n",
      "Epoch: 2, Train loss: 2.069, Val loss: 1.830, Epoch time = 80.000s\n",
      "BLEU-4 corpus score = 0.10264180181678066, corpus length = 1000, time elapsed = 17.991553783416748 seconds.\n",
      "Epoch: 3, Train loss: 1.642, Val loss: 1.532, Epoch time = 136.918s\n",
      "BLEU-4 corpus score = 0.19192645517598933, corpus length = 1000, time elapsed = 11.6562979221344 seconds.\n",
      "Epoch: 4, Train loss: 1.381, Val loss: 1.344, Epoch time = 187.618s\n",
      "BLEU-4 corpus score = 0.27025180251916203, corpus length = 1000, time elapsed = 7.1950085163116455 seconds.\n",
      "Epoch: 5, Train loss: 1.222, Val loss: 1.238, Epoch time = 234.547s\n",
      "BLEU-4 corpus score = 0.29783702794389627, corpus length = 1000, time elapsed = 9.538561582565308 seconds.\n",
      "Epoch: 6, Train loss: 1.111, Val loss: 1.191, Epoch time = 283.278s\n",
      "BLEU-4 corpus score = 0.3001576731083972, corpus length = 1000, time elapsed = 9.459630966186523 seconds.\n",
      "Epoch: 7, Train loss: 1.028, Val loss: 1.148, Epoch time = 331.653s\n",
      "BLEU-4 corpus score = 0.3334924689939816, corpus length = 1000, time elapsed = 5.997708797454834 seconds.\n",
      "Epoch: 8, Train loss: 0.958, Val loss: 1.121, Epoch time = 376.564s\n",
      "BLEU-4 corpus score = 0.3422507882849572, corpus length = 1000, time elapsed = 4.62183952331543 seconds.\n",
      "Epoch: 9, Train loss: 0.897, Val loss: 1.096, Epoch time = 420.152s\n",
      "BLEU-4 corpus score = 0.35912792286575895, corpus length = 1000, time elapsed = 4.75954532623291 seconds.\n",
      "Epoch: 10, Train loss: 0.844, Val loss: 1.084, Epoch time = 466.326s\n",
      "BLEU-4 corpus score = 0.3415859891326656, corpus length = 1000, time elapsed = 5.358043909072876 seconds.\n",
      "Epoch: 11, Train loss: 0.798, Val loss: 1.070, Epoch time = 511.376s\n",
      "BLEU-4 corpus score = 0.3665726783630465, corpus length = 1000, time elapsed = 3.9652323722839355 seconds.\n",
      "Epoch: 12, Train loss: 0.763, Val loss: 1.078, Epoch time = 554.896s\n",
      "BLEU-4 corpus score = 0.3475850806789388, corpus length = 1000, time elapsed = 5.550133228302002 seconds.\n",
      "Epoch: 13, Train loss: 0.734, Val loss: 1.076, Epoch time = 599.368s\n",
      "BLEU-4 corpus score = 0.35366158693327304, corpus length = 1000, time elapsed = 5.333411931991577 seconds.\n",
      "Epoch: 14, Train loss: 0.712, Val loss: 1.069, Epoch time = 643.507s\n",
      "BLEU-4 corpus score = 0.35180722236786316, corpus length = 1000, time elapsed = 6.475532054901123 seconds.\n",
      "Epoch: 15, Train loss: 0.679, Val loss: 1.078, Epoch time = 688.972s\n",
      "BLEU-4 corpus score = 0.34562596157975733, corpus length = 1000, time elapsed = 6.744523525238037 seconds.\n",
      "Epoch: 16, Train loss: 0.667, Val loss: 1.070, Epoch time = 734.698s\n",
      "BLEU-4 corpus score = 0.3559298495993372, corpus length = 1000, time elapsed = 6.939112901687622 seconds.\n",
      "Epoch: 17, Train loss: 0.657, Val loss: 1.071, Epoch time = 781.509s\n",
      "BLEU-4 corpus score = 0.3647275480968903, corpus length = 1000, time elapsed = 6.377941608428955 seconds.\n",
      "Epoch: 18, Train loss: 0.639, Val loss: 1.064, Epoch time = 826.960s\n",
      "BLEU-4 corpus score = 0.3658995216256636, corpus length = 1000, time elapsed = 5.5112526416778564 seconds.\n",
      "Epoch: 19, Train loss: 0.625, Val loss: 1.074, Epoch time = 871.613s\n",
      "BLEU-4 corpus score = 0.36161834167028023, corpus length = 1000, time elapsed = 6.377746820449829 seconds.\n",
      "Epoch: 20, Train loss: 0.614, Val loss: 1.069, Epoch time = 916.939s\n",
      "BLEU-4 corpus score = 0.3523926773191926, corpus length = 1000, time elapsed = 6.912848949432373 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.500, Val loss: 2.505, Epoch time = 37.701s\n",
      "BLEU-4 corpus score = 0.04425601253192197, corpus length = 1000, time elapsed = 1.1453404426574707 seconds.\n",
      "Epoch: 2, Train loss: 2.063, Val loss: 1.853, Epoch time = 76.577s\n",
      "BLEU-4 corpus score = 0.1189692390235628, corpus length = 1000, time elapsed = 5.225606679916382 seconds.\n",
      "Epoch: 3, Train loss: 1.664, Val loss: 1.568, Epoch time = 119.581s\n",
      "BLEU-4 corpus score = 0.20854563541368099, corpus length = 1000, time elapsed = 6.108285665512085 seconds.\n",
      "Epoch: 4, Train loss: 1.436, Val loss: 1.398, Epoch time = 163.485s\n",
      "BLEU-4 corpus score = 0.25312815838400576, corpus length = 1000, time elapsed = 10.377304792404175 seconds.\n",
      "Epoch: 5, Train loss: 1.278, Val loss: 1.287, Epoch time = 211.543s\n",
      "BLEU-4 corpus score = 0.2922526952233995, corpus length = 1000, time elapsed = 6.265305042266846 seconds.\n",
      "Epoch: 6, Train loss: 1.158, Val loss: 1.231, Epoch time = 256.443s\n",
      "BLEU-4 corpus score = 0.31436149293823035, corpus length = 1000, time elapsed = 4.542523622512817 seconds.\n",
      "Epoch: 7, Train loss: 1.082, Val loss: 1.192, Epoch time = 298.866s\n",
      "BLEU-4 corpus score = 0.3189678354430138, corpus length = 1000, time elapsed = 5.710803031921387 seconds.\n",
      "Epoch: 8, Train loss: 1.010, Val loss: 1.152, Epoch time = 342.801s\n",
      "BLEU-4 corpus score = 0.32135390404537745, corpus length = 1000, time elapsed = 8.503129243850708 seconds.\n",
      "Epoch: 9, Train loss: 0.940, Val loss: 1.111, Epoch time = 389.079s\n",
      "BLEU-4 corpus score = 0.3327120466802106, corpus length = 1000, time elapsed = 6.693873882293701 seconds.\n",
      "Epoch: 10, Train loss: 0.887, Val loss: 1.102, Epoch time = 433.608s\n",
      "BLEU-4 corpus score = 0.35121639559505663, corpus length = 1000, time elapsed = 5.981083631515503 seconds.\n",
      "Epoch: 11, Train loss: 0.841, Val loss: 1.082, Epoch time = 477.446s\n",
      "BLEU-4 corpus score = 0.34086612082621215, corpus length = 1000, time elapsed = 7.992861747741699 seconds.\n",
      "Epoch: 12, Train loss: 0.808, Val loss: 1.076, Epoch time = 523.275s\n",
      "BLEU-4 corpus score = 0.36357506785910815, corpus length = 1000, time elapsed = 6.973381757736206 seconds.\n",
      "Epoch: 13, Train loss: 0.781, Val loss: 1.071, Epoch time = 567.991s\n",
      "BLEU-4 corpus score = 0.36429609731007967, corpus length = 1000, time elapsed = 4.614217042922974 seconds.\n",
      "Epoch: 14, Train loss: 0.756, Val loss: 1.068, Epoch time = 610.413s\n",
      "BLEU-4 corpus score = 0.3522923908252005, corpus length = 1000, time elapsed = 4.780011177062988 seconds.\n",
      "Epoch: 15, Train loss: 0.727, Val loss: 1.061, Epoch time = 653.054s\n",
      "BLEU-4 corpus score = 0.3486862665188418, corpus length = 1000, time elapsed = 5.986114263534546 seconds.\n",
      "Epoch: 16, Train loss: 0.701, Val loss: 1.067, Epoch time = 696.796s\n",
      "BLEU-4 corpus score = 0.35664354310798746, corpus length = 1000, time elapsed = 4.760017156600952 seconds.\n",
      "Epoch: 17, Train loss: 0.695, Val loss: 1.069, Epoch time = 739.323s\n",
      "BLEU-4 corpus score = 0.36414366276115157, corpus length = 1000, time elapsed = 4.732213020324707 seconds.\n",
      "Epoch: 18, Train loss: 0.674, Val loss: 1.074, Epoch time = 781.944s\n",
      "BLEU-4 corpus score = 0.3665749411307088, corpus length = 1000, time elapsed = 4.658921241760254 seconds.\n",
      "Epoch: 19, Train loss: 0.663, Val loss: 1.072, Epoch time = 824.654s\n",
      "BLEU-4 corpus score = 0.36628643135923383, corpus length = 1000, time elapsed = 7.653184652328491 seconds.\n",
      "Epoch: 20, Train loss: 0.651, Val loss: 1.061, Epoch time = 870.144s\n",
      "BLEU-4 corpus score = 0.3566162773036527, corpus length = 1000, time elapsed = 8.373986721038818 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.508, Val loss: 2.527, Epoch time = 37.806s\n",
      "BLEU-4 corpus score = 0.04563911508649442, corpus length = 1000, time elapsed = 1.538938283920288 seconds.\n",
      "Epoch: 2, Train loss: 2.072, Val loss: 1.861, Epoch time = 77.141s\n",
      "BLEU-4 corpus score = 0.10886258684609734, corpus length = 1000, time elapsed = 11.91013765335083 seconds.\n",
      "Epoch: 3, Train loss: 1.668, Val loss: 1.575, Epoch time = 126.904s\n",
      "BLEU-4 corpus score = 0.2032685709862835, corpus length = 1000, time elapsed = 5.953350305557251 seconds.\n",
      "Epoch: 4, Train loss: 1.429, Val loss: 1.394, Epoch time = 170.700s\n",
      "BLEU-4 corpus score = 0.22748306712211705, corpus length = 1000, time elapsed = 17.031941652297974 seconds.\n",
      "Epoch: 5, Train loss: 1.270, Val loss: 1.293, Epoch time = 225.555s\n",
      "BLEU-4 corpus score = 0.3050284706083505, corpus length = 1000, time elapsed = 3.3055849075317383 seconds.\n",
      "Epoch: 6, Train loss: 1.151, Val loss: 1.235, Epoch time = 266.677s\n",
      "BLEU-4 corpus score = 0.3288983109858479, corpus length = 1000, time elapsed = 4.552177667617798 seconds.\n",
      "Epoch: 7, Train loss: 1.080, Val loss: 1.197, Epoch time = 309.022s\n",
      "BLEU-4 corpus score = 0.31208283937491094, corpus length = 1000, time elapsed = 3.3045530319213867 seconds.\n",
      "Epoch: 8, Train loss: 1.014, Val loss: 1.154, Epoch time = 350.136s\n",
      "BLEU-4 corpus score = 0.3395421817322644, corpus length = 1000, time elapsed = 6.842940807342529 seconds.\n",
      "Epoch: 9, Train loss: 0.941, Val loss: 1.128, Epoch time = 394.831s\n",
      "BLEU-4 corpus score = 0.3382747118130558, corpus length = 1000, time elapsed = 4.672109127044678 seconds.\n",
      "Epoch: 10, Train loss: 0.889, Val loss: 1.100, Epoch time = 437.390s\n",
      "BLEU-4 corpus score = 0.3563750970165263, corpus length = 1000, time elapsed = 5.832260608673096 seconds.\n",
      "Epoch: 11, Train loss: 0.853, Val loss: 1.095, Epoch time = 481.035s\n",
      "BLEU-4 corpus score = 0.36360913626745606, corpus length = 1000, time elapsed = 3.3151237964630127 seconds.\n",
      "Epoch: 12, Train loss: 0.807, Val loss: 1.085, Epoch time = 522.212s\n",
      "BLEU-4 corpus score = 0.3563111651389776, corpus length = 1000, time elapsed = 4.8243513107299805 seconds.\n",
      "Epoch: 13, Train loss: 0.785, Val loss: 1.077, Epoch time = 564.890s\n",
      "BLEU-4 corpus score = 0.35466076507553984, corpus length = 1000, time elapsed = 7.690438747406006 seconds.\n",
      "Epoch: 14, Train loss: 0.758, Val loss: 1.080, Epoch time = 610.488s\n",
      "BLEU-4 corpus score = 0.36050483574152553, corpus length = 1000, time elapsed = 6.4754650592803955 seconds.\n",
      "Epoch: 15, Train loss: 0.734, Val loss: 1.079, Epoch time = 654.796s\n",
      "BLEU-4 corpus score = 0.34953275566088754, corpus length = 1000, time elapsed = 7.3308916091918945 seconds.\n",
      "Epoch: 16, Train loss: 0.711, Val loss: 1.076, Epoch time = 699.974s\n",
      "BLEU-4 corpus score = 0.3666716236044659, corpus length = 1000, time elapsed = 4.86878776550293 seconds.\n",
      "Epoch: 17, Train loss: 0.702, Val loss: 1.074, Epoch time = 742.596s\n",
      "BLEU-4 corpus score = 0.37264095629722876, corpus length = 1000, time elapsed = 5.568555116653442 seconds.\n",
      "Epoch: 18, Train loss: 0.686, Val loss: 1.080, Epoch time = 786.015s\n",
      "BLEU-4 corpus score = 0.3625984193547987, corpus length = 1000, time elapsed = 5.7144012451171875 seconds.\n",
      "Epoch: 19, Train loss: 0.665, Val loss: 1.073, Epoch time = 829.583s\n",
      "BLEU-4 corpus score = 0.345619649525139, corpus length = 1000, time elapsed = 6.876499652862549 seconds.\n",
      "Epoch: 20, Train loss: 0.656, Val loss: 1.076, Epoch time = 874.221s\n",
      "BLEU-4 corpus score = 0.3705906379185338, corpus length = 1000, time elapsed = 6.564778804779053 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.505, Val loss: 2.528, Epoch time = 37.915s\n",
      "BLEU-4 corpus score = 0.05274695553969579, corpus length = 1000, time elapsed = 3.0715441703796387 seconds.\n",
      "Epoch: 2, Train loss: 2.066, Val loss: 1.818, Epoch time = 78.961s\n",
      "BLEU-4 corpus score = 0.11904328177224158, corpus length = 1000, time elapsed = 6.602013111114502 seconds.\n",
      "Epoch: 3, Train loss: 1.628, Val loss: 1.483, Epoch time = 123.501s\n",
      "BLEU-4 corpus score = 0.20492808506777332, corpus length = 1000, time elapsed = 9.428157329559326 seconds.\n",
      "Epoch: 4, Train loss: 1.373, Val loss: 1.272, Epoch time = 170.791s\n",
      "BLEU-4 corpus score = 0.2913515518697836, corpus length = 1000, time elapsed = 4.465969085693359 seconds.\n",
      "Epoch: 5, Train loss: 1.176, Val loss: 1.148, Epoch time = 213.110s\n",
      "BLEU-4 corpus score = 0.3141916237134131, corpus length = 1000, time elapsed = 5.7596259117126465 seconds.\n",
      "Epoch: 6, Train loss: 1.050, Val loss: 1.083, Epoch time = 256.881s\n",
      "BLEU-4 corpus score = 0.33876320518698977, corpus length = 1000, time elapsed = 3.6864140033721924 seconds.\n",
      "Epoch: 7, Train loss: 0.973, Val loss: 1.062, Epoch time = 298.494s\n",
      "BLEU-4 corpus score = 0.35503342974514523, corpus length = 1000, time elapsed = 3.6514370441436768 seconds.\n",
      "Epoch: 8, Train loss: 0.893, Val loss: 1.018, Epoch time = 340.132s\n",
      "BLEU-4 corpus score = 0.33936181004899535, corpus length = 1000, time elapsed = 7.788839817047119 seconds.\n",
      "Epoch: 9, Train loss: 0.824, Val loss: 1.002, Epoch time = 385.918s\n",
      "BLEU-4 corpus score = 0.37639723018179694, corpus length = 1000, time elapsed = 3.929440498352051 seconds.\n",
      "Epoch: 10, Train loss: 0.775, Val loss: 1.003, Epoch time = 427.737s\n",
      "BLEU-4 corpus score = 0.36488416372037374, corpus length = 1000, time elapsed = 5.656677484512329 seconds.\n",
      "Epoch: 11, Train loss: 0.735, Val loss: 0.975, Epoch time = 471.286s\n",
      "BLEU-4 corpus score = 0.3532053275542935, corpus length = 1000, time elapsed = 7.7157142162323 seconds.\n",
      "Epoch: 12, Train loss: 0.694, Val loss: 0.965, Epoch time = 517.121s\n",
      "BLEU-4 corpus score = 0.37071943794410617, corpus length = 1000, time elapsed = 4.579662084579468 seconds.\n",
      "Epoch: 13, Train loss: 0.663, Val loss: 0.969, Epoch time = 559.735s\n",
      "BLEU-4 corpus score = 0.361590707975266, corpus length = 1000, time elapsed = 6.787378311157227 seconds.\n",
      "Epoch: 14, Train loss: 0.644, Val loss: 0.972, Epoch time = 604.500s\n",
      "BLEU-4 corpus score = 0.3603942312960265, corpus length = 1000, time elapsed = 6.706970930099487 seconds.\n",
      "Epoch: 15, Train loss: 0.623, Val loss: 0.974, Epoch time = 649.118s\n",
      "BLEU-4 corpus score = 0.38081822945367305, corpus length = 1000, time elapsed = 4.57805323600769 seconds.\n",
      "Epoch: 16, Train loss: 0.604, Val loss: 0.976, Epoch time = 691.676s\n",
      "BLEU-4 corpus score = 0.3710252032884596, corpus length = 1000, time elapsed = 4.958529949188232 seconds.\n",
      "Epoch: 17, Train loss: 0.586, Val loss: 0.975, Epoch time = 734.602s\n",
      "BLEU-4 corpus score = 0.3677533726933693, corpus length = 1000, time elapsed = 5.686180830001831 seconds.\n",
      "Epoch: 18, Train loss: 0.570, Val loss: 0.973, Epoch time = 778.202s\n",
      "BLEU-4 corpus score = 0.366268248160811, corpus length = 1000, time elapsed = 5.883742094039917 seconds.\n",
      "Epoch: 19, Train loss: 0.564, Val loss: 0.975, Epoch time = 822.072s\n",
      "BLEU-4 corpus score = 0.3859975450642239, corpus length = 1000, time elapsed = 3.8147153854370117 seconds.\n",
      "Epoch: 20, Train loss: 0.550, Val loss: 0.973, Epoch time = 863.818s\n",
      "BLEU-4 corpus score = 0.3642188611160032, corpus length = 1000, time elapsed = 4.834240674972534 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.488, Val loss: 2.543, Epoch time = 37.935s\n",
      "BLEU-4 corpus score = 0.04369967745831116, corpus length = 1000, time elapsed = 1.349151849746704 seconds.\n",
      "Epoch: 2, Train loss: 2.097, Val loss: 1.844, Epoch time = 77.270s\n",
      "BLEU-4 corpus score = 0.1232920811430023, corpus length = 1000, time elapsed = 4.181628942489624 seconds.\n",
      "Epoch: 3, Train loss: 1.642, Val loss: 1.494, Epoch time = 119.456s\n",
      "BLEU-4 corpus score = 0.218424927411919, corpus length = 1000, time elapsed = 5.367701768875122 seconds.\n",
      "Epoch: 4, Train loss: 1.367, Val loss: 1.268, Epoch time = 162.760s\n",
      "BLEU-4 corpus score = 0.2820899199432822, corpus length = 1000, time elapsed = 4.727303504943848 seconds.\n",
      "Epoch: 5, Train loss: 1.168, Val loss: 1.164, Epoch time = 205.494s\n",
      "BLEU-4 corpus score = 0.3230096686476002, corpus length = 1000, time elapsed = 3.6784749031066895 seconds.\n",
      "Epoch: 6, Train loss: 1.047, Val loss: 1.083, Epoch time = 247.110s\n",
      "BLEU-4 corpus score = 0.3339380266348732, corpus length = 1000, time elapsed = 5.669737815856934 seconds.\n",
      "Epoch: 7, Train loss: 0.965, Val loss: 1.060, Epoch time = 290.769s\n",
      "BLEU-4 corpus score = 0.3576187758992564, corpus length = 1000, time elapsed = 3.504351854324341 seconds.\n",
      "Epoch: 8, Train loss: 0.899, Val loss: 1.017, Epoch time = 332.197s\n",
      "BLEU-4 corpus score = 0.3521648275189619, corpus length = 1000, time elapsed = 3.744140386581421 seconds.\n",
      "Epoch: 9, Train loss: 0.823, Val loss: 0.998, Epoch time = 373.953s\n",
      "BLEU-4 corpus score = 0.3584813830497892, corpus length = 1000, time elapsed = 6.966394901275635 seconds.\n",
      "Epoch: 10, Train loss: 0.775, Val loss: 0.985, Epoch time = 418.804s\n",
      "BLEU-4 corpus score = 0.36608928116385503, corpus length = 1000, time elapsed = 6.732104063034058 seconds.\n",
      "Epoch: 11, Train loss: 0.732, Val loss: 0.983, Epoch time = 463.428s\n",
      "BLEU-4 corpus score = 0.3740960197538121, corpus length = 1000, time elapsed = 4.643836498260498 seconds.\n",
      "Epoch: 12, Train loss: 0.697, Val loss: 0.980, Epoch time = 505.971s\n",
      "BLEU-4 corpus score = 0.36434107616226097, corpus length = 1000, time elapsed = 5.041443347930908 seconds.\n",
      "Epoch: 13, Train loss: 0.668, Val loss: 0.968, Epoch time = 548.896s\n",
      "BLEU-4 corpus score = 0.3719826614605141, corpus length = 1000, time elapsed = 4.680510520935059 seconds.\n",
      "Epoch: 14, Train loss: 0.639, Val loss: 0.969, Epoch time = 591.573s\n",
      "BLEU-4 corpus score = 0.366205219313299, corpus length = 1000, time elapsed = 5.692941665649414 seconds.\n",
      "Epoch: 15, Train loss: 0.619, Val loss: 0.972, Epoch time = 635.246s\n",
      "BLEU-4 corpus score = 0.37889051659480827, corpus length = 1000, time elapsed = 4.746680736541748 seconds.\n",
      "Epoch: 16, Train loss: 0.602, Val loss: 0.974, Epoch time = 677.919s\n",
      "BLEU-4 corpus score = 0.368649451498018, corpus length = 1000, time elapsed = 5.6083502769470215 seconds.\n",
      "Epoch: 17, Train loss: 0.585, Val loss: 0.975, Epoch time = 721.485s\n",
      "BLEU-4 corpus score = 0.3610490962678967, corpus length = 1000, time elapsed = 6.895273208618164 seconds.\n",
      "Epoch: 18, Train loss: 0.572, Val loss: 0.978, Epoch time = 766.332s\n",
      "BLEU-4 corpus score = 0.38388759168156744, corpus length = 1000, time elapsed = 5.089778661727905 seconds.\n",
      "Epoch: 19, Train loss: 0.564, Val loss: 0.978, Epoch time = 809.358s\n",
      "BLEU-4 corpus score = 0.37976853955561135, corpus length = 1000, time elapsed = 5.016727924346924 seconds.\n",
      "Epoch: 20, Train loss: 0.551, Val loss: 0.971, Epoch time = 852.338s\n",
      "BLEU-4 corpus score = 0.37443573186673357, corpus length = 1000, time elapsed = 4.907575845718384 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.514, Val loss: 2.549, Epoch time = 40.382s\n",
      "BLEU-4 corpus score = 0.04060240086702162, corpus length = 1000, time elapsed = 1.3612895011901855 seconds.\n",
      "Epoch: 2, Train loss: 2.116, Val loss: 1.906, Epoch time = 82.091s\n",
      "BLEU-4 corpus score = 0.0972741837207087, corpus length = 1000, time elapsed = 20.136445999145508 seconds.\n",
      "Epoch: 3, Train loss: 1.685, Val loss: 1.574, Epoch time = 142.642s\n",
      "BLEU-4 corpus score = 0.18012661880342054, corpus length = 1000, time elapsed = 10.783063411712646 seconds.\n",
      "Epoch: 4, Train loss: 1.403, Val loss: 1.378, Epoch time = 193.808s\n",
      "BLEU-4 corpus score = 0.26339989134688, corpus length = 1000, time elapsed = 8.218770027160645 seconds.\n",
      "Epoch: 5, Train loss: 1.232, Val loss: 1.270, Epoch time = 242.370s\n",
      "BLEU-4 corpus score = 0.3134460423640159, corpus length = 1000, time elapsed = 6.241464138031006 seconds.\n",
      "Epoch: 6, Train loss: 1.102, Val loss: 1.203, Epoch time = 288.953s\n",
      "BLEU-4 corpus score = 0.3301834632224196, corpus length = 1000, time elapsed = 6.977936744689941 seconds.\n",
      "Epoch: 7, Train loss: 1.013, Val loss: 1.160, Epoch time = 336.647s\n",
      "BLEU-4 corpus score = 0.3424678001183507, corpus length = 1000, time elapsed = 4.655979633331299 seconds.\n",
      "Epoch: 8, Train loss: 0.950, Val loss: 1.144, Epoch time = 381.625s\n",
      "BLEU-4 corpus score = 0.3549475644706884, corpus length = 1000, time elapsed = 4.05503249168396 seconds.\n",
      "Epoch: 9, Train loss: 0.883, Val loss: 1.099, Epoch time = 426.059s\n",
      "BLEU-4 corpus score = 0.3587133598671395, corpus length = 1000, time elapsed = 6.671180248260498 seconds.\n",
      "Epoch: 10, Train loss: 0.818, Val loss: 1.088, Epoch time = 473.178s\n",
      "BLEU-4 corpus score = 0.35226003478068213, corpus length = 1000, time elapsed = 8.144737243652344 seconds.\n",
      "Epoch: 11, Train loss: 0.784, Val loss: 1.079, Epoch time = 521.640s\n",
      "BLEU-4 corpus score = 0.3587904693276832, corpus length = 1000, time elapsed = 5.582602500915527 seconds.\n",
      "Epoch: 12, Train loss: 0.748, Val loss: 1.067, Epoch time = 567.575s\n",
      "BLEU-4 corpus score = 0.3558051865957277, corpus length = 1000, time elapsed = 5.652471303939819 seconds.\n",
      "Epoch: 13, Train loss: 0.714, Val loss: 1.078, Epoch time = 613.645s\n",
      "BLEU-4 corpus score = 0.36739014176503365, corpus length = 1000, time elapsed = 5.66998291015625 seconds.\n",
      "Epoch: 14, Train loss: 0.688, Val loss: 1.059, Epoch time = 659.716s\n",
      "BLEU-4 corpus score = 0.3530339751165422, corpus length = 1000, time elapsed = 6.439836025238037 seconds.\n",
      "Epoch: 15, Train loss: 0.668, Val loss: 1.064, Epoch time = 706.464s\n",
      "BLEU-4 corpus score = 0.3707852785348707, corpus length = 1000, time elapsed = 4.360565185546875 seconds.\n",
      "Epoch: 16, Train loss: 0.647, Val loss: 1.058, Epoch time = 751.227s\n",
      "BLEU-4 corpus score = 0.35209512581912966, corpus length = 1000, time elapsed = 9.008432388305664 seconds.\n",
      "Epoch: 17, Train loss: 0.628, Val loss: 1.066, Epoch time = 800.572s\n",
      "BLEU-4 corpus score = 0.3766690705308039, corpus length = 1000, time elapsed = 5.404872179031372 seconds.\n",
      "Epoch: 18, Train loss: 0.625, Val loss: 1.052, Epoch time = 846.366s\n",
      "BLEU-4 corpus score = 0.3589460089943524, corpus length = 1000, time elapsed = 8.772515296936035 seconds.\n",
      "Epoch: 19, Train loss: 0.604, Val loss: 1.065, Epoch time = 895.556s\n",
      "BLEU-4 corpus score = 0.34731673518552225, corpus length = 1000, time elapsed = 7.199338912963867 seconds.\n",
      "Epoch: 20, Train loss: 0.591, Val loss: 1.064, Epoch time = 943.168s\n",
      "BLEU-4 corpus score = 0.36104870528078736, corpus length = 1000, time elapsed = 7.659851551055908 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.500, Val loss: 2.521, Epoch time = 40.469s\n",
      "BLEU-4 corpus score = 0.04978335724893293, corpus length = 1000, time elapsed = 1.918748378753662 seconds.\n",
      "Epoch: 2, Train loss: 2.064, Val loss: 1.876, Epoch time = 82.940s\n",
      "BLEU-4 corpus score = 0.08135021776130973, corpus length = 1000, time elapsed = 18.98946762084961 seconds.\n",
      "Epoch: 3, Train loss: 1.688, Val loss: 1.602, Epoch time = 142.470s\n",
      "BLEU-4 corpus score = 0.16419522540104384, corpus length = 1000, time elapsed = 11.059122323989868 seconds.\n",
      "Epoch: 4, Train loss: 1.464, Val loss: 1.432, Epoch time = 193.938s\n",
      "BLEU-4 corpus score = 0.23072405891943684, corpus length = 1000, time elapsed = 8.227983236312866 seconds.\n",
      "Epoch: 5, Train loss: 1.292, Val loss: 1.329, Epoch time = 242.750s\n",
      "BLEU-4 corpus score = 0.25431799195645255, corpus length = 1000, time elapsed = 13.39283013343811 seconds.\n",
      "Epoch: 6, Train loss: 1.186, Val loss: 1.254, Epoch time = 296.595s\n",
      "BLEU-4 corpus score = 0.2754258739834678, corpus length = 1000, time elapsed = 11.235580205917358 seconds.\n",
      "Epoch: 7, Train loss: 1.100, Val loss: 1.222, Epoch time = 348.299s\n",
      "BLEU-4 corpus score = 0.2840963666817048, corpus length = 1000, time elapsed = 11.042814493179321 seconds.\n",
      "Epoch: 8, Train loss: 1.028, Val loss: 1.186, Epoch time = 399.778s\n",
      "BLEU-4 corpus score = 0.30997447094921976, corpus length = 1000, time elapsed = 5.383767366409302 seconds.\n",
      "Epoch: 9, Train loss: 0.965, Val loss: 1.165, Epoch time = 445.626s\n",
      "BLEU-4 corpus score = 0.29171415568061637, corpus length = 1000, time elapsed = 7.184796333312988 seconds.\n",
      "Epoch: 10, Train loss: 0.915, Val loss: 1.138, Epoch time = 493.175s\n",
      "BLEU-4 corpus score = 0.31149339780676905, corpus length = 1000, time elapsed = 8.855068922042847 seconds.\n",
      "Epoch: 11, Train loss: 0.864, Val loss: 1.126, Epoch time = 542.471s\n",
      "BLEU-4 corpus score = 0.3197391456928492, corpus length = 1000, time elapsed = 6.254019021987915 seconds.\n",
      "Epoch: 12, Train loss: 0.826, Val loss: 1.125, Epoch time = 589.198s\n",
      "BLEU-4 corpus score = 0.3183568052861003, corpus length = 1000, time elapsed = 5.019850015640259 seconds.\n",
      "Epoch: 13, Train loss: 0.794, Val loss: 1.117, Epoch time = 634.670s\n",
      "BLEU-4 corpus score = 0.3267003371330455, corpus length = 1000, time elapsed = 4.284775733947754 seconds.\n",
      "Epoch: 14, Train loss: 0.771, Val loss: 1.110, Epoch time = 679.402s\n",
      "BLEU-4 corpus score = 0.33542670156732873, corpus length = 1000, time elapsed = 6.137409210205078 seconds.\n",
      "Epoch: 15, Train loss: 0.747, Val loss: 1.109, Epoch time = 725.946s\n",
      "BLEU-4 corpus score = 0.3322504861538326, corpus length = 1000, time elapsed = 5.642360687255859 seconds.\n",
      "Epoch: 16, Train loss: 0.726, Val loss: 1.115, Epoch time = 772.014s\n",
      "BLEU-4 corpus score = 0.335628887463289, corpus length = 1000, time elapsed = 5.9437079429626465 seconds.\n",
      "Epoch: 17, Train loss: 0.714, Val loss: 1.114, Epoch time = 818.404s\n",
      "BLEU-4 corpus score = 0.3430498017559601, corpus length = 1000, time elapsed = 4.442010879516602 seconds.\n",
      "Epoch: 18, Train loss: 0.691, Val loss: 1.123, Epoch time = 863.279s\n",
      "BLEU-4 corpus score = 0.33853010559336977, corpus length = 1000, time elapsed = 4.4698944091796875 seconds.\n",
      "Epoch: 19, Train loss: 0.680, Val loss: 1.115, Epoch time = 908.103s\n",
      "BLEU-4 corpus score = 0.3291448217367024, corpus length = 1000, time elapsed = 7.2778379917144775 seconds.\n",
      "Epoch: 20, Train loss: 0.660, Val loss: 1.122, Epoch time = 955.840s\n",
      "BLEU-4 corpus score = 0.3355123572405854, corpus length = 1000, time elapsed = 4.6147260665893555 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.512, Val loss: 2.506, Epoch time = 38.566s\n",
      "BLEU-4 corpus score = 0.04844341858639727, corpus length = 1000, time elapsed = 1.7063915729522705 seconds.\n",
      "Epoch: 2, Train loss: 2.053, Val loss: 1.841, Epoch time = 78.813s\n",
      "BLEU-4 corpus score = 0.10214470837920082, corpus length = 1000, time elapsed = 11.58159613609314 seconds.\n",
      "Epoch: 3, Train loss: 1.635, Val loss: 1.527, Epoch time = 129.080s\n",
      "BLEU-4 corpus score = 0.20287909030932155, corpus length = 1000, time elapsed = 3.80370831489563 seconds.\n",
      "Epoch: 4, Train loss: 1.380, Val loss: 1.337, Epoch time = 171.582s\n",
      "BLEU-4 corpus score = 0.27616771459991607, corpus length = 1000, time elapsed = 5.133166790008545 seconds.\n",
      "Epoch: 5, Train loss: 1.215, Val loss: 1.243, Epoch time = 215.270s\n",
      "BLEU-4 corpus score = 0.31323666054785093, corpus length = 1000, time elapsed = 3.4459195137023926 seconds.\n",
      "Epoch: 6, Train loss: 1.098, Val loss: 1.191, Epoch time = 257.350s\n",
      "BLEU-4 corpus score = 0.3256667095427757, corpus length = 1000, time elapsed = 4.7244017124176025 seconds.\n",
      "Epoch: 7, Train loss: 1.031, Val loss: 1.138, Epoch time = 300.670s\n",
      "BLEU-4 corpus score = 0.3415731550392162, corpus length = 1000, time elapsed = 7.325836658477783 seconds.\n",
      "Epoch: 8, Train loss: 0.955, Val loss: 1.097, Epoch time = 346.619s\n",
      "BLEU-4 corpus score = 0.3427266058391254, corpus length = 1000, time elapsed = 7.140131235122681 seconds.\n",
      "Epoch: 9, Train loss: 0.893, Val loss: 1.073, Epoch time = 392.420s\n",
      "BLEU-4 corpus score = 0.3576619156933791, corpus length = 1000, time elapsed = 4.9576096534729 seconds.\n",
      "Epoch: 10, Train loss: 0.835, Val loss: 1.056, Epoch time = 436.008s\n",
      "BLEU-4 corpus score = 0.3599108218057145, corpus length = 1000, time elapsed = 6.327016353607178 seconds.\n",
      "Epoch: 11, Train loss: 0.792, Val loss: 1.052, Epoch time = 481.218s\n",
      "BLEU-4 corpus score = 0.3525466895683163, corpus length = 1000, time elapsed = 6.0703675746917725 seconds.\n",
      "Epoch: 12, Train loss: 0.759, Val loss: 1.042, Epoch time = 525.995s\n",
      "BLEU-4 corpus score = 0.3621062925201149, corpus length = 1000, time elapsed = 6.339481353759766 seconds.\n",
      "Epoch: 13, Train loss: 0.728, Val loss: 1.040, Epoch time = 570.997s\n",
      "BLEU-4 corpus score = 0.3530551861105402, corpus length = 1000, time elapsed = 5.576297760009766 seconds.\n",
      "Epoch: 14, Train loss: 0.709, Val loss: 1.038, Epoch time = 615.095s\n",
      "BLEU-4 corpus score = 0.37131925540791644, corpus length = 1000, time elapsed = 6.048438310623169 seconds.\n",
      "Epoch: 15, Train loss: 0.686, Val loss: 1.039, Epoch time = 659.743s\n",
      "BLEU-4 corpus score = 0.38132385604528446, corpus length = 1000, time elapsed = 5.877830982208252 seconds.\n",
      "Epoch: 16, Train loss: 0.670, Val loss: 1.038, Epoch time = 704.256s\n",
      "BLEU-4 corpus score = 0.3694724691635743, corpus length = 1000, time elapsed = 5.158953905105591 seconds.\n",
      "Epoch: 17, Train loss: 0.657, Val loss: 1.033, Epoch time = 747.976s\n",
      "BLEU-4 corpus score = 0.37903883299799485, corpus length = 1000, time elapsed = 5.932213068008423 seconds.\n",
      "Epoch: 18, Train loss: 0.642, Val loss: 1.032, Epoch time = 792.490s\n",
      "BLEU-4 corpus score = 0.36761547351534996, corpus length = 1000, time elapsed = 7.728721618652344 seconds.\n",
      "Epoch: 19, Train loss: 0.628, Val loss: 1.029, Epoch time = 838.870s\n",
      "BLEU-4 corpus score = 0.38027986866965197, corpus length = 1000, time elapsed = 4.238501787185669 seconds.\n",
      "Epoch: 20, Train loss: 0.612, Val loss: 1.035, Epoch time = 881.718s\n",
      "BLEU-4 corpus score = 0.37063217238880625, corpus length = 1000, time elapsed = 6.297549247741699 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.506, Val loss: 2.516, Epoch time = 38.625s\n",
      "BLEU-4 corpus score = 0.04425601253192197, corpus length = 1000, time elapsed = 1.2032415866851807 seconds.\n",
      "Epoch: 2, Train loss: 2.063, Val loss: 1.835, Epoch time = 78.463s\n",
      "BLEU-4 corpus score = 0.10683815404855995, corpus length = 1000, time elapsed = 12.437940120697021 seconds.\n",
      "Epoch: 3, Train loss: 1.640, Val loss: 1.516, Epoch time = 129.571s\n",
      "BLEU-4 corpus score = 0.19395665364251033, corpus length = 1000, time elapsed = 10.110296964645386 seconds.\n",
      "Epoch: 4, Train loss: 1.375, Val loss: 1.321, Epoch time = 178.432s\n",
      "BLEU-4 corpus score = 0.2714916434629364, corpus length = 1000, time elapsed = 9.604720115661621 seconds.\n",
      "Epoch: 5, Train loss: 1.211, Val loss: 1.223, Epoch time = 226.690s\n",
      "BLEU-4 corpus score = 0.32092077912304545, corpus length = 1000, time elapsed = 3.655385971069336 seconds.\n",
      "Epoch: 6, Train loss: 1.098, Val loss: 1.181, Epoch time = 268.925s\n",
      "BLEU-4 corpus score = 0.3432393324344583, corpus length = 1000, time elapsed = 5.639758348464966 seconds.\n",
      "Epoch: 7, Train loss: 1.019, Val loss: 1.138, Epoch time = 313.198s\n",
      "BLEU-4 corpus score = 0.3274623646313037, corpus length = 1000, time elapsed = 5.275360822677612 seconds.\n",
      "Epoch: 8, Train loss: 0.953, Val loss: 1.103, Epoch time = 357.186s\n",
      "BLEU-4 corpus score = 0.3472374640347211, corpus length = 1000, time elapsed = 8.129167318344116 seconds.\n",
      "Epoch: 9, Train loss: 0.891, Val loss: 1.072, Epoch time = 403.923s\n",
      "BLEU-4 corpus score = 0.3685312180227848, corpus length = 1000, time elapsed = 4.721756935119629 seconds.\n",
      "Epoch: 10, Train loss: 0.836, Val loss: 1.055, Epoch time = 447.300s\n",
      "BLEU-4 corpus score = 0.3536369210080452, corpus length = 1000, time elapsed = 7.198479175567627 seconds.\n",
      "Epoch: 11, Train loss: 0.793, Val loss: 1.039, Epoch time = 493.174s\n",
      "BLEU-4 corpus score = 0.3534017425701959, corpus length = 1000, time elapsed = 9.08624529838562 seconds.\n",
      "Epoch: 12, Train loss: 0.766, Val loss: 1.045, Epoch time = 540.875s\n",
      "BLEU-4 corpus score = 0.3429381742197877, corpus length = 1000, time elapsed = 7.99416446685791 seconds.\n",
      "Epoch: 13, Train loss: 0.728, Val loss: 1.034, Epoch time = 587.564s\n",
      "BLEU-4 corpus score = 0.36096286339160244, corpus length = 1000, time elapsed = 6.547104835510254 seconds.\n",
      "Epoch: 14, Train loss: 0.707, Val loss: 1.036, Epoch time = 632.768s\n",
      "BLEU-4 corpus score = 0.368073656482, corpus length = 1000, time elapsed = 7.389718055725098 seconds.\n",
      "Epoch: 15, Train loss: 0.681, Val loss: 1.034, Epoch time = 678.828s\n",
      "BLEU-4 corpus score = 0.3568142951350513, corpus length = 1000, time elapsed = 8.257452249526978 seconds.\n",
      "Epoch: 16, Train loss: 0.670, Val loss: 1.034, Epoch time = 725.732s\n",
      "BLEU-4 corpus score = 0.3785425635398448, corpus length = 1000, time elapsed = 5.825814723968506 seconds.\n",
      "Epoch: 17, Train loss: 0.651, Val loss: 1.035, Epoch time = 770.465s\n",
      "BLEU-4 corpus score = 0.374811807594985, corpus length = 1000, time elapsed = 5.108441114425659 seconds.\n",
      "Epoch: 18, Train loss: 0.639, Val loss: 1.036, Epoch time = 814.211s\n",
      "BLEU-4 corpus score = 0.3714068974419848, corpus length = 1000, time elapsed = 4.6224634647369385 seconds.\n",
      "Epoch: 19, Train loss: 0.627, Val loss: 1.034, Epoch time = 857.493s\n",
      "BLEU-4 corpus score = 0.38384956038453927, corpus length = 1000, time elapsed = 4.0681610107421875 seconds.\n",
      "Epoch: 20, Train loss: 0.613, Val loss: 1.031, Epoch time = 900.219s\n",
      "BLEU-4 corpus score = 0.384170697403155, corpus length = 1000, time elapsed = 4.1448798179626465 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.523, Val loss: 2.528, Epoch time = 38.676s\n",
      "BLEU-4 corpus score = 0.04256798023368803, corpus length = 1000, time elapsed = 1.401641607284546 seconds.\n",
      "Epoch: 2, Train loss: 2.068, Val loss: 1.826, Epoch time = 78.815s\n",
      "BLEU-4 corpus score = 0.08754992441898564, corpus length = 1000, time elapsed = 18.348946809768677 seconds.\n",
      "Epoch: 3, Train loss: 1.612, Val loss: 1.466, Epoch time = 135.983s\n",
      "BLEU-4 corpus score = 0.2230560288893265, corpus length = 1000, time elapsed = 3.682901382446289 seconds.\n",
      "Epoch: 4, Train loss: 1.326, Val loss: 1.250, Epoch time = 178.419s\n",
      "BLEU-4 corpus score = 0.28220759360262326, corpus length = 1000, time elapsed = 8.10219407081604 seconds.\n",
      "Epoch: 5, Train loss: 1.142, Val loss: 1.122, Epoch time = 225.260s\n",
      "BLEU-4 corpus score = 0.3234448045498145, corpus length = 1000, time elapsed = 4.049081325531006 seconds.\n",
      "Epoch: 6, Train loss: 1.013, Val loss: 1.057, Epoch time = 268.127s\n",
      "BLEU-4 corpus score = 0.34570093257728474, corpus length = 1000, time elapsed = 5.844186305999756 seconds.\n",
      "Epoch: 7, Train loss: 0.933, Val loss: 1.027, Epoch time = 312.694s\n",
      "BLEU-4 corpus score = 0.35737699853421273, corpus length = 1000, time elapsed = 3.864241361618042 seconds.\n",
      "Epoch: 8, Train loss: 0.856, Val loss: 0.986, Epoch time = 355.386s\n",
      "BLEU-4 corpus score = 0.3564330537494773, corpus length = 1000, time elapsed = 5.948759317398071 seconds.\n",
      "Epoch: 9, Train loss: 0.795, Val loss: 0.968, Epoch time = 400.065s\n",
      "BLEU-4 corpus score = 0.36455705264916555, corpus length = 1000, time elapsed = 3.9886281490325928 seconds.\n",
      "Epoch: 10, Train loss: 0.736, Val loss: 0.965, Epoch time = 442.830s\n",
      "BLEU-4 corpus score = 0.3709952547089059, corpus length = 1000, time elapsed = 5.063207149505615 seconds.\n",
      "Epoch: 11, Train loss: 0.696, Val loss: 0.957, Epoch time = 486.629s\n",
      "BLEU-4 corpus score = 0.3761112507743983, corpus length = 1000, time elapsed = 4.153492212295532 seconds.\n",
      "Epoch: 12, Train loss: 0.663, Val loss: 0.951, Epoch time = 529.429s\n",
      "BLEU-4 corpus score = 0.3852680772393202, corpus length = 1000, time elapsed = 5.185004949569702 seconds.\n",
      "Epoch: 13, Train loss: 0.632, Val loss: 0.946, Epoch time = 573.409s\n",
      "BLEU-4 corpus score = 0.38028606829004225, corpus length = 1000, time elapsed = 4.110002517700195 seconds.\n",
      "Epoch: 14, Train loss: 0.608, Val loss: 0.943, Epoch time = 616.209s\n",
      "BLEU-4 corpus score = 0.3809092060054428, corpus length = 1000, time elapsed = 5.200140476226807 seconds.\n",
      "Epoch: 15, Train loss: 0.589, Val loss: 0.951, Epoch time = 660.093s\n",
      "BLEU-4 corpus score = 0.3833979556132057, corpus length = 1000, time elapsed = 4.1311256885528564 seconds.\n",
      "Epoch: 16, Train loss: 0.564, Val loss: 0.952, Epoch time = 703.041s\n",
      "BLEU-4 corpus score = 0.36864821821008437, corpus length = 1000, time elapsed = 8.059117317199707 seconds.\n",
      "Epoch: 17, Train loss: 0.549, Val loss: 0.951, Epoch time = 749.931s\n",
      "BLEU-4 corpus score = 0.37901498720999677, corpus length = 1000, time elapsed = 4.722831726074219 seconds.\n",
      "Epoch: 18, Train loss: 0.539, Val loss: 0.951, Epoch time = 793.402s\n",
      "BLEU-4 corpus score = 0.38441669797649586, corpus length = 1000, time elapsed = 4.039267063140869 seconds.\n",
      "Epoch: 19, Train loss: 0.527, Val loss: 0.951, Epoch time = 836.246s\n",
      "BLEU-4 corpus score = 0.3849890567818749, corpus length = 1000, time elapsed = 4.088929176330566 seconds.\n",
      "Epoch: 20, Train loss: 0.515, Val loss: 0.951, Epoch time = 879.134s\n",
      "BLEU-4 corpus score = 0.3821282873054982, corpus length = 1000, time elapsed = 4.027467966079712 seconds.\n",
      "translation_en_de---------- norm type: qkv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.505, Val loss: 2.530, Epoch time = 38.823s\n",
      "BLEU-4 corpus score = 0.05072035165386622, corpus length = 1000, time elapsed = 1.7911138534545898 seconds.\n",
      "Epoch: 2, Train loss: 2.086, Val loss: 1.826, Epoch time = 79.418s\n",
      "BLEU-4 corpus score = 0.10638030311077892, corpus length = 1000, time elapsed = 10.470930814743042 seconds.\n",
      "Epoch: 3, Train loss: 1.623, Val loss: 1.466, Epoch time = 128.673s\n",
      "BLEU-4 corpus score = 0.1950958833352353, corpus length = 1000, time elapsed = 12.41273832321167 seconds.\n",
      "Epoch: 4, Train loss: 1.322, Val loss: 1.241, Epoch time = 179.965s\n",
      "BLEU-4 corpus score = 0.2809962521251325, corpus length = 1000, time elapsed = 7.115499258041382 seconds.\n",
      "Epoch: 5, Train loss: 1.137, Val loss: 1.122, Epoch time = 225.989s\n",
      "BLEU-4 corpus score = 0.3266096882811237, corpus length = 1000, time elapsed = 4.831124782562256 seconds.\n",
      "Epoch: 6, Train loss: 1.016, Val loss: 1.061, Epoch time = 269.703s\n",
      "BLEU-4 corpus score = 0.3499180266870718, corpus length = 1000, time elapsed = 7.111475229263306 seconds.\n",
      "Epoch: 7, Train loss: 0.928, Val loss: 1.024, Epoch time = 315.770s\n",
      "BLEU-4 corpus score = 0.33193133490279925, corpus length = 1000, time elapsed = 9.662949085235596 seconds.\n",
      "Epoch: 8, Train loss: 0.864, Val loss: 0.988, Epoch time = 364.270s\n",
      "BLEU-4 corpus score = 0.3625731051824949, corpus length = 1000, time elapsed = 4.836637735366821 seconds.\n",
      "Epoch: 9, Train loss: 0.794, Val loss: 0.973, Epoch time = 407.994s\n",
      "BLEU-4 corpus score = 0.3758261423036833, corpus length = 1000, time elapsed = 5.121683120727539 seconds.\n",
      "Epoch: 10, Train loss: 0.737, Val loss: 0.960, Epoch time = 452.013s\n",
      "BLEU-4 corpus score = 0.3762165030603046, corpus length = 1000, time elapsed = 7.657414436340332 seconds.\n",
      "Epoch: 11, Train loss: 0.692, Val loss: 0.952, Epoch time = 498.562s\n",
      "BLEU-4 corpus score = 0.3616085397683713, corpus length = 1000, time elapsed = 7.5967912673950195 seconds.\n",
      "Epoch: 12, Train loss: 0.666, Val loss: 0.954, Epoch time = 545.019s\n",
      "BLEU-4 corpus score = 0.3692672055194211, corpus length = 1000, time elapsed = 8.15286111831665 seconds.\n",
      "Epoch: 13, Train loss: 0.635, Val loss: 0.955, Epoch time = 591.982s\n",
      "BLEU-4 corpus score = 0.3760169951604575, corpus length = 1000, time elapsed = 7.185031414031982 seconds.\n",
      "Epoch: 14, Train loss: 0.608, Val loss: 0.951, Epoch time = 638.065s\n",
      "BLEU-4 corpus score = 0.3914133147874223, corpus length = 1000, time elapsed = 6.320603370666504 seconds.\n",
      "Epoch: 15, Train loss: 0.586, Val loss: 0.954, Epoch time = 683.769s\n",
      "BLEU-4 corpus score = 0.3739683551787291, corpus length = 1000, time elapsed = 7.561740398406982 seconds.\n",
      "Epoch: 16, Train loss: 0.570, Val loss: 0.955, Epoch time = 730.181s\n",
      "BLEU-4 corpus score = 0.3760744908678392, corpus length = 1000, time elapsed = 9.385467767715454 seconds.\n",
      "Epoch: 17, Train loss: 0.555, Val loss: 0.960, Epoch time = 778.477s\n",
      "BLEU-4 corpus score = 0.3759201597624883, corpus length = 1000, time elapsed = 7.161054611206055 seconds.\n",
      "Epoch: 18, Train loss: 0.536, Val loss: 0.957, Epoch time = 824.470s\n",
      "BLEU-4 corpus score = 0.3778239919443436, corpus length = 1000, time elapsed = 7.177734136581421 seconds.\n",
      "Epoch: 19, Train loss: 0.527, Val loss: 0.960, Epoch time = 870.615s\n",
      "BLEU-4 corpus score = 0.38486575900798115, corpus length = 1000, time elapsed = 7.107447624206543 seconds.\n",
      "Epoch: 20, Train loss: 0.513, Val loss: 0.961, Epoch time = 916.637s\n",
      "BLEU-4 corpus score = 0.3765697479931566, corpus length = 1000, time elapsed = 7.73615837097168 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.275, Val loss: 2.292, Epoch time = 46.063s\n",
      "BLEU-4 corpus score = 0.07085719149813272, corpus length = 1000, time elapsed = 3.6174962520599365 seconds.\n",
      "Epoch: 2, Train loss: 1.945, Val loss: 1.821, Epoch time = 95.924s\n",
      "BLEU-4 corpus score = 0.1186675216693747, corpus length = 1000, time elapsed = 8.3655686378479 seconds.\n",
      "Epoch: 3, Train loss: 1.619, Val loss: 1.532, Epoch time = 150.402s\n",
      "BLEU-4 corpus score = 0.19790396501975346, corpus length = 1000, time elapsed = 6.652820587158203 seconds.\n",
      "Epoch: 4, Train loss: 1.367, Val loss: 1.361, Epoch time = 203.272s\n",
      "BLEU-4 corpus score = 0.2434553465916409, corpus length = 1000, time elapsed = 17.716962337493896 seconds.\n",
      "Epoch: 5, Train loss: 1.202, Val loss: 1.270, Epoch time = 267.411s\n",
      "BLEU-4 corpus score = 0.2540636571285376, corpus length = 1000, time elapsed = 5.34783411026001 seconds.\n",
      "Epoch: 6, Train loss: 1.097, Val loss: 1.204, Epoch time = 318.882s\n",
      "BLEU-4 corpus score = 0.29042159846670773, corpus length = 1000, time elapsed = 4.5417704582214355 seconds.\n",
      "Epoch: 7, Train loss: 1.020, Val loss: 1.198, Epoch time = 369.602s\n",
      "BLEU-4 corpus score = 0.30047279937156474, corpus length = 1000, time elapsed = 4.479659080505371 seconds.\n",
      "Epoch: 8, Train loss: 0.947, Val loss: 1.145, Epoch time = 420.207s\n",
      "BLEU-4 corpus score = 0.3188862174180628, corpus length = 1000, time elapsed = 10.322028398513794 seconds.\n",
      "Epoch: 9, Train loss: 0.864, Val loss: 1.118, Epoch time = 477.102s\n",
      "BLEU-4 corpus score = 0.31771949145879874, corpus length = 1000, time elapsed = 13.438422918319702 seconds.\n",
      "Epoch: 10, Train loss: 0.809, Val loss: 1.113, Epoch time = 536.838s\n",
      "BLEU-4 corpus score = 0.3336241580081869, corpus length = 1000, time elapsed = 6.667584419250488 seconds.\n",
      "Epoch: 11, Train loss: 0.755, Val loss: 1.100, Epoch time = 589.701s\n",
      "BLEU-4 corpus score = 0.3308010264464378, corpus length = 1000, time elapsed = 11.90700626373291 seconds.\n",
      "Epoch: 12, Train loss: 0.720, Val loss: 1.104, Epoch time = 647.692s\n",
      "BLEU-4 corpus score = 0.33642966804816626, corpus length = 1000, time elapsed = 9.66180968284607 seconds.\n",
      "Epoch: 13, Train loss: 0.674, Val loss: 1.125, Epoch time = 703.696s\n",
      "BLEU-4 corpus score = 0.3311359423740691, corpus length = 1000, time elapsed = 7.353551864624023 seconds.\n",
      "Epoch: 14, Train loss: 0.647, Val loss: 1.113, Epoch time = 757.210s\n",
      "BLEU-4 corpus score = 0.3310530220181881, corpus length = 1000, time elapsed = 7.293136358261108 seconds.\n",
      "Epoch: 15, Train loss: 0.622, Val loss: 1.110, Epoch time = 810.627s\n",
      "BLEU-4 corpus score = 0.3289781804551311, corpus length = 1000, time elapsed = 7.473581552505493 seconds.\n",
      "Epoch: 16, Train loss: 0.603, Val loss: 1.109, Epoch time = 864.174s\n",
      "BLEU-4 corpus score = 0.34469858330112996, corpus length = 1000, time elapsed = 5.008699178695679 seconds.\n",
      "Epoch: 17, Train loss: 0.577, Val loss: 1.116, Epoch time = 915.373s\n",
      "BLEU-4 corpus score = 0.3404162466469196, corpus length = 1000, time elapsed = 5.058518886566162 seconds.\n",
      "Epoch: 18, Train loss: 0.556, Val loss: 1.115, Epoch time = 966.675s\n",
      "BLEU-4 corpus score = 0.336681556686016, corpus length = 1000, time elapsed = 7.687260866165161 seconds.\n",
      "Epoch: 19, Train loss: 0.540, Val loss: 1.126, Epoch time = 1020.519s\n",
      "BLEU-4 corpus score = 0.3434675876697066, corpus length = 1000, time elapsed = 4.833820104598999 seconds.\n",
      "Epoch: 20, Train loss: 0.527, Val loss: 1.129, Epoch time = 1071.490s\n",
      "BLEU-4 corpus score = 0.3457952838049141, corpus length = 1000, time elapsed = 7.024829626083374 seconds.\n",
      "translation_en_de---------- norm type: kv, run number: 1 ------------------------------\n",
      "Epoch: 1, Train loss: 3.256, Val loss: 2.295, Epoch time = 46.167s\n",
      "BLEU-4 corpus score = 0.05506340293896589, corpus length = 1000, time elapsed = 3.6273562908172607 seconds.\n",
      "Epoch: 2, Train loss: 1.940, Val loss: 1.767, Epoch time = 96.115s\n",
      "BLEU-4 corpus score = 0.12674502670179408, corpus length = 1000, time elapsed = 11.730069398880005 seconds.\n",
      "Epoch: 3, Train loss: 1.591, Val loss: 1.502, Epoch time = 154.076s\n",
      "BLEU-4 corpus score = 0.19647608448228826, corpus length = 1000, time elapsed = 17.46849775314331 seconds.\n",
      "Epoch: 4, Train loss: 1.365, Val loss: 1.363, Epoch time = 217.778s\n",
      "BLEU-4 corpus score = 0.27334349125860763, corpus length = 1000, time elapsed = 4.271817207336426 seconds.\n",
      "Epoch: 5, Train loss: 1.199, Val loss: 1.271, Epoch time = 268.476s\n",
      "BLEU-4 corpus score = 0.2721736960982059, corpus length = 1000, time elapsed = 16.90153193473816 seconds.\n",
      "Epoch: 6, Train loss: 1.099, Val loss: 1.213, Epoch time = 331.579s\n",
      "BLEU-4 corpus score = 0.3086558760087944, corpus length = 1000, time elapsed = 4.168851137161255 seconds.\n",
      "Epoch: 7, Train loss: 1.016, Val loss: 1.192, Epoch time = 381.965s\n",
      "BLEU-4 corpus score = 0.3038822299322145, corpus length = 1000, time elapsed = 7.9731457233428955 seconds.\n",
      "Epoch: 8, Train loss: 0.939, Val loss: 1.152, Epoch time = 435.986s\n",
      "BLEU-4 corpus score = 0.32089414967963154, corpus length = 1000, time elapsed = 5.360591173171997 seconds.\n",
      "Epoch: 9, Train loss: 0.871, Val loss: 1.139, Epoch time = 487.375s\n",
      "BLEU-4 corpus score = 0.3210613921296303, corpus length = 1000, time elapsed = 12.48122763633728 seconds.\n",
      "Epoch: 10, Train loss: 0.796, Val loss: 1.119, Epoch time = 546.079s\n",
      "BLEU-4 corpus score = 0.33612677207242986, corpus length = 1000, time elapsed = 9.210956335067749 seconds.\n",
      "Epoch: 11, Train loss: 0.748, Val loss: 1.106, Epoch time = 601.386s\n",
      "BLEU-4 corpus score = 0.34582419874475334, corpus length = 1000, time elapsed = 4.934427499771118 seconds.\n",
      "Epoch: 12, Train loss: 0.704, Val loss: 1.100, Epoch time = 652.399s\n",
      "BLEU-4 corpus score = 0.3297225049323831, corpus length = 1000, time elapsed = 12.108105897903442 seconds.\n",
      "Epoch: 13, Train loss: 0.665, Val loss: 1.110, Epoch time = 710.569s\n",
      "BLEU-4 corpus score = 0.3497313291807782, corpus length = 1000, time elapsed = 6.603894472122192 seconds.\n",
      "Epoch: 14, Train loss: 0.636, Val loss: 1.105, Epoch time = 763.144s\n",
      "BLEU-4 corpus score = 0.3416392793252599, corpus length = 1000, time elapsed = 7.289915323257446 seconds.\n",
      "Epoch: 15, Train loss: 0.609, Val loss: 1.116, Epoch time = 816.517s\n",
      "BLEU-4 corpus score = 0.34256679467536827, corpus length = 1000, time elapsed = 6.6375648975372314 seconds.\n",
      "Epoch: 16, Train loss: 0.582, Val loss: 1.120, Epoch time = 869.306s\n",
      "BLEU-4 corpus score = 0.35357177050504635, corpus length = 1000, time elapsed = 4.603257656097412 seconds.\n",
      "Epoch: 17, Train loss: 0.561, Val loss: 1.125, Epoch time = 920.052s\n",
      "BLEU-4 corpus score = 0.34099429852039614, corpus length = 1000, time elapsed = 4.89124870300293 seconds.\n",
      "Epoch: 18, Train loss: 0.540, Val loss: 1.125, Epoch time = 971.053s\n",
      "BLEU-4 corpus score = 0.34637503265162956, corpus length = 1000, time elapsed = 8.238150358200073 seconds.\n",
      "Epoch: 19, Train loss: 0.529, Val loss: 1.136, Epoch time = 1025.458s\n",
      "BLEU-4 corpus score = 0.3481751905102405, corpus length = 1000, time elapsed = 6.547879934310913 seconds.\n",
      "Epoch: 20, Train loss: 0.518, Val loss: 1.130, Epoch time = 1078.011s\n",
      "BLEU-4 corpus score = 0.34895452903472923, corpus length = 1000, time elapsed = 6.82229208946228 seconds.\n",
      "translation_en_de---------- norm type: kv-nopos, run number: 0 ------------------------------\n",
      "Epoch: 1, Train loss: 3.267, Val loss: 2.287, Epoch time = 44.969s\n",
      "BLEU-4 corpus score = 0.07211293458467702, corpus length = 1000, time elapsed = 2.7634365558624268 seconds.\n",
      "Epoch: 2, Train loss: 1.929, Val loss: 1.755, Epoch time = 92.712s\n"
     ]
    }
   ],
   "source": [
    "for BASELINE_MODEL_NUMBER_OF_LAYERS in [2]: #[1, 2]:\n",
    "  for BASELINE_MODEL_DIMENSION in [64, 128, 256]:\n",
    "      for BASELINE_MODEL_NUMBER_OF_HEADS in [1, 4]:\n",
    "\n",
    "            results = {}\n",
    "\n",
    "            # for norm_type in ['no-sm-std', 'no-sm-layernorm']:\n",
    "            for attn_type in ['kv', 'kv-nopos', 'qkv']:\n",
    "\n",
    "              for run_no in range(num_runs):\n",
    "\n",
    "                  print(f'{which_task}---------- norm type: {attn_type}, run number: {run_no} ------------------------------')\n",
    "\n",
    "                  file_name = f'{BASELINE_MODEL_NUMBER_OF_LAYERS}_{BASELINE_MODEL_DIMENSION}_\\\n",
    "                  {BASELINE_MODEL_NUMBER_OF_HEADS}_{BASELINE_MODEL_DROPOUT_PROB}_{which_task}'\n",
    "\n",
    "                  transformer, kl_div_loss, optimizer, custom_lr_optimizer = define_model()\n",
    "\n",
    "                  results[(attn_type, run_no)] = {}\n",
    "                  results[(attn_type, run_no)]['time_spent'] = []  \n",
    "                  results[(attn_type, run_no)]['train_loss_history'] = []  \n",
    "                  results[(attn_type, run_no)]['val_loss_history'] = []  \n",
    "                  results[(attn_type, run_no)]['val_time_spent'] = []  \n",
    "                  results[(attn_type, run_no)]['test_acc'] = []  \n",
    "\n",
    "                  start_time = time.time()\n",
    "\n",
    "                  for epoch in range(1, NUM_EPOCHS+1):\n",
    "                        \n",
    "                        train_loss = train_epoch(transformer, optimizer)\n",
    "                        results[(attn_type, run_no)]['time_spent'].append(time.time() - start_time) \n",
    "\n",
    "                        val_loss = evaluate(transformer)\n",
    "                        results[(attn_type, run_no)]['val_time_spent'].append(time.time() - start_time) \n",
    "\n",
    "                        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(time.time() - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "                        results[(attn_type, run_no)]['train_loss_history'].append(train_loss)\n",
    "                        results[(attn_type, run_no)]['val_loss_history'].append(val_loss)\n",
    "\n",
    "                        bleu_score = calculate_bleu_score(transformer, test_dataloader, vocab_transform[TGT_LANGUAGE])\n",
    "\n",
    "\n",
    "                        results[(attn_type, run_no)]['test_acc'].append(bleu_score)\n",
    "\n",
    "\n",
    "                  with open(f'./results_new/nlp/{file_name}.pkl', 'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # reverse_model\n",
    "# input = torch.randn(1,16,10).to(device)\n",
    "# macs, params = profile(reverse_model, inputs=(input, ))\n",
    "# print(macs/1e6, params/1e6)\n",
    "\n",
    "# plt.imshow(attention[0,0].cpu().numpy()); plt.show()\n",
    "# plt.imshow(attention[0,0].detach().cpu().numpy()); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDKehPDlSPjq"
   },
   "source": [
    "Now we have all the ingredients to train our model. Let's do it!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gS0XUxUHSPjq"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    bleu_score = calculate_bleu_score(transformer, test_dataloader, vocab_transform[TGT_LANGUAGE])\n",
    "    # print(bleu_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(scores[0,0].detach().cpu().numpy()); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)std\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEEAHD5wSPjr"
   },
   "outputs": [],
   "source": [
    "# print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(translate(transformer, \"Two men are sitting in front of a wall.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "losses = 0\n",
    "\n",
    "val_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "all_ref = []\n",
    "all_cand = []\n",
    "\n",
    "for src, tgt in val_dataloader:\n",
    "    # src = src.to(DEVICE)\n",
    "    for idx in range(src.shape[-1]):\n",
    "        srs_sent = \" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[:,idx].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
    "        tgt_sent = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[:,idx].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
    "        trans = translate(transformer, srs_sent)\n",
    "\n",
    "        # print(f' src: {srs_sent}\\n target: {tgt_sent}\\n translation: {trans}\\n')\n",
    "\n",
    "        if not tgt_sent.strip(): continue\n",
    "\n",
    "        all_ref.append(tgt_sent.split()) \n",
    "        all_cand.append(trans)\n",
    "      \n",
    "        # print('one more batch')\n",
    "        \n",
    "bleu = corpus_bleu(all_ref, all_cand)*100\n",
    "print(bleu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not tgt_sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cand[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bleu(all_ref[1000:1014], all_cand[1000:1014])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8_YDHUnSPjr"
   },
   "source": [
    "## References\n",
    "\n",
    "1. Attention is all you need paper.\n",
    "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
